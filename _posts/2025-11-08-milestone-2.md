---
layout: post
title: Milestone 2
---
## Introduction: How to Predict a Goal?

Following the NHL data exploration of *Milestone 1*, we now dive deeper into the task of estimating the **quality of shots**. For this, we calculate the likelihood of  shot resulting in a goal (**Expected Goals**) based on available features, e.g. *shot type*, *distance*, *angle*, etc. This exercise consists of six steps:

- Step 1 introduces a first round of **feature engineering (I)** to prepare the dataset for linear regression models.
- Step 2 runs initial tests with **linear regression**.
- Step 3 presents a second round of **feature engineering (II)** to create advanced features for future models.
- Step 4 tries different **hyperparameter tuning** and **feature selection** methods to determine the best parameters and features, and trains three **XGBoost** models (see Figures 4.1…).
- Step 5 proposes the **best model** based on previous experiments.
- Step 6 presents the **test results**.

All experiments and the most relevant models are logged on our [Weights & Biases page](https://wandb.ai/IFT6758_team4/milestone_2) (WB).



---


# **Step 1: Feature Engineering I**

{% include milestone2/distance_histogram.html %}
discuss
{% include milestone2/angle_histogram.html %}
discuss
{% include milestone2/distance_angle_histogram.html %}
discuss
{% include milestone2/goalrate_distance.html %}
discuss
{% include milestone2/goalrate_angle.html %}
discuss
{% include milestone2/goals_distance.html %}
discuss
incorrect features

---

# **Step 2: Baseline Models**

we trained three baseline **logistic regressions** on **distance only**, **angle only**, and **distance+angle**, plus a **random-uniform** baseline. evaluation used **validation probabilities** only.

**setup.** features: **distance_from_net**, **shot_angle**; target: **is_goal** (1 = goal). from `baseline_train.csv` we made a **stratified 80/20 split (seed 42)** and trained three default logistic regressions: (i) distance, (ii) angle, (iii) distance+angle. we also included a random-uniform baseline. missing values in the two features were imputed with the **median fitted on the training split**. the **test set was not used**.

**evaluation.** because we care about **probabilities** (expected goals), all curves use **validation-set predicted probabilities for class 1 (goal)**:

1. **roc curve (+ auc)** with a **chance (45°)** line
2. **goal rate vs model percentile**
3. **cumulative % of goals vs percentile**
4. **reliability (calibration) diagram**

**Findings:**

* **distance+angle** gives the best ranking (**AUC ≈ 0.707**), **distance** is close behind (**AUC ≈ 0.692**), **angle** is weak (**AUC ≈ 0.560**), and **random-uniform** sits at chance (**AUC ≈ 0.496**).
* high-probability percentiles have much higher **goal rates**; **distance+angle** stays above the others across most of the curve, with **distance** next and **angle** trailing; **random** is nearly flat.
* in the cumulative plot, **distance+angle** captures goals fastest, then **distance**, then **angle**; **random** increases almost linearly.
* calibration isn’t perfect: most predictions lie in a low-probability band, and points deviate from the diagonal (some under/over-confidence), but the ranking remains useful.

---

## **Question 2.1**

we split the data into training and validation using a stratified split so the goal rate is preserved. we then trained a default **logistic regression** on a single feature, **distance_from_net**, and evaluated **accuracy** on the validation set. the accuracy looks deceptively decent, but inspection of the predictions shows most probabilities are very small; with a 0.5 threshold, the model predicts almost all shots as “no goal.” because goals are relatively rare, a model can achieve high accuracy without actually identifying goals. **takeaway:** accuracy is a poor metric here; we should evaluate the **predicted probabilities** directly rather than hard 0/1 decisions.

## **Question 2.2**

motivated by q2.1, we use `predict_proba` and evaluate **validation probabilities** for the positive class (goal). we generated four diagnostics: roc/auc, goal-rate vs percentile, cumulative % of goals vs percentile, and a reliability diagram. these show that probability ranking is informative even when raw accuracy is misleading.

## **Question 2.3**

we repeated the setup for **angle only** and **distance+angle**, and added a **random-uniform** baseline. we produced the **same four figures** on the validation set, each with **four curves**.

* **roc/auc:** **distance+angle (≈ 0.707)** > **distance (≈ 0.692)** >> **angle (≈ 0.560)** > **random (≈ 0.496)**.
* **goal rate vs percentile:** top percentiles show the highest goal rates; ordering is **distance+angle** > **distance** > **angle** > **random** (random is nearly flat).
* **cumulative % of goals:** **distance+angle** accumulates goals fastest across percentiles, confirming stronger ranking than single-feature models.
* **reliability:** models are not perfectly calibrated; most predictions live around low probabilities, with some bins showing under/over-confidence (especially at higher predicted values where sample sizes are smaller).

**takeaway:** **distance** carries most of the signal; **adding angle** improves ranking further; **random** behaves as expected.

---

## **Question 2.4**

{% include milestone2/q2_fig_roc_auc.html width="600" %}

**Figure — ROC (+ AUC, ranking quality).** *lr-both* (**AUC ≈ 0.707**) ranks shots best; *lr-distance* (**AUC ≈ 0.692**) is next; *lr-angle* (**AUC ≈ 0.560**) is only slightly above chance; **random-uniform** (**AUC ≈ 0.496**) tracks the **chance (45°)** line.

---

{% include milestone2/q2_fig_goalrate_vs_percentile.html width="600" %}

**Figure — Goal rate vs model percentile.** Goal rate peaks at the **top percentiles** and declines steadily. Ordering across most of the curve is **lr-both** > **lr-distance** > **lr-angle**; **random-uniform** is relatively flat.

---

{% include milestone2/q2_fig_cum_goals_vs_percentile.html width="600" %}

**Figure — Cumulative % of goals vs percentile.** **lr-both** rises fastest (captures goals earliest), **lr-distance** follows closely, **lr-angle** lags, and **random-uniform** increases near linearly.

---

{% include milestone2/q2_fig_reliability_diagram.html width="600" %}

**Figure — Reliability (calibration) diagram.** Most predictions sit in a **low-probability band**; deviations from the diagonal indicate **imperfect calibration** (some under/over-confidence), which is typical for simple baselines. Despite this, ranking remains useful, as confirmed by the ROC and percentile plots.

--

- **Artifacts & runs.** The three models are saved as W&B **artifacts**, and each experiment run is linked below:

### Baseline Runs
- [lr-distance (run)](https://wandb.ai/IFT6758_team4/milestone_2/runs/26ipd8pc?nw=nwuseraftabgazali003)
- [lr-angle (run)](https://wandb.ai/IFT6758_team4/milestone_2/runs/4qcqb2hw?nw=nwuseraftabgazali003)
- [lr-both (run)](https://wandb.ai/IFT6758_team4/milestone_2/runs/3g7slfrs?nw=nwuseraftabgazali003)

### Saved Baseline Models (Artifacts)
- [lr-distance:v1](https://wandb.ai/IFT6758_team4/milestone_2/artifacts/model/lr-distance/v1)
- [lr-angle:v1](https://wandb.ai/IFT6758_team4/milestone_2/artifacts/model/lr-angle/v1)
- [lr-both:v1](https://wandb.ai/IFT6758_team4/milestone_2/artifacts/model/lr-both/v1)

**Note.** The **test set remains untouched** in this section; everything above uses the training/validation split only.


---
# **Step 3: Feature Engineering II**

## Question 3.1 :
## Question 3.2 :
## Question 3.3 :
## Question 3.4 :
## Question 3.5 :

<!-- ![Figure 3.1](assets/images/screenshot.png) -->

- *Figure 3.1 - *


---

# **Step 4: Advanced Models**
In this exercise, we test three approaches to training XGBoost classifiers for predicting the most dangerous shots. Our first baseline model (1) predicts the likelihood of a shot resulting in a goal based only on its *distance* and *shot angle*, without any hyperparameter fine-tuning. The second model (2A, using two input features) includes only numerical features, while the third model (2B) incorporates all 14 available features. For both, we apply hyperparameter fine-tuning with gread search and cross-validation. We then aim to identify the relevant, redundant, and weak features using information-theory methods. We compute information gain for each feature and mutual information for each pair to make a first selection, and then confirms the selection through both forward and backward wrappers. We end up selecting seven features as the most relevant and train our final model (3) on this set.

Since XGBoost can handle missing values natively, we ensure that all missing entries are formatted as np.nan and rely on the model’s internal mechanisms for handling them. For each model tested, we present the same four figures used in Question 3: ROC/AUC, Goal rate vs. probability percentile, Cumulative proportion of goals vs. probability percentile, and Reliability curve.


## Question 4.1: Baseline XGBoost classifier

**Model-1**


<table>
  <tr>
    <td>{% include milestone2/roc_model1.html width="400" %}</td>
    <td>{% include milestone2/percentile_model1.html width="400" %}</td>
  </tr>
  <tr>
    <td>{% include milestone2/cumulative_model1.html width="400" %}</td>
    <td>{% include milestone2/calibration_model1.html width="400" %}</td>
  </tr>
</table>


- *Figure 4.1. - The four figures for the baseline model (Model-1). The ROC curve shows an AUC of ~0.71 for the validation set. This indicates that the model can distinguish to some degree higher-quality shots from low-quality ones since it performs better than a random guess would (AUC=0.5).*

The percentile curve (top right) behaves as expected as the actual goal rate increases for shots in bins with higher predicted probabilities. For example, among the 10% of shots the model identified as most dangerous, 22.5% were actual goals. Similarly, the top 20% of shots by predicted xG account for 40% of all goals, and the top 40% account for 75% of all goals (bottom left). However, the calibration curve (bottom right) indicates that shots the model predicted as more likely to score actually scored less often than those with lower predicted probabilities.\
See the relevant WB log entry [here](https://wandb.ai/IFT6758_team4/milestone_2/runs/3j35f7mq?nw=nwuserantonioslagarias).

## Question 4.2: Full XGBoost classifier

XGBoost can handle both numerical and categorical features. To assess its performance on datasets containing a mix of feature types — such as our *advanced_data* — we test two models. First, we exclude the two categorical features, *shot type* and *previous event type* (model 2A, with 12 features). Then, we include them in model 2B, which has access to all 14 features. For both models, we optimize the hyperparameters using the same grid search approach with cross-validation (CV).

**Model-2A**

For model-2A, we ran cross-validation three times, adjusting the grid, optimizing for AUC. First, a 3-fold CV gave us the best configuration as (max_depth=5, eta=0.1, min_child_weight=5, subsample=1.0, colsample_bytree=0.8) with an AUC of 0.733 (73 rounds). A second 5-fold CV gave us (max_depth=5, eta=0.05, min_child_weight=1, subsample=1.0, colsample_bytree=0.8) with an AUC of 0.7334 (99 rounds). The third 5-fold CV produced the same configuration (max_depth=5, eta=0.05, min_child_weight=1, subsample=1.0, colsample_bytree=0.8) with an AUC of 0.7336 (122 rounds), which we kept for training (see figure 4.2).\
See the relevant WB log entry [here](https://wandb.ai/IFT6758_team4/milestone_2/runs/escltr3d/artifacts?nw=nwuserantonioslagarias).


<table>
  <tr>
    <td>{% include milestone2/roc_model2A.html width="400" %}</td>
    <td>{% include milestone2/percentile_model2A.html width="400" %}</td>
  </tr>
  <tr>
    <td>{% include milestone2/cumulative_model2A.html width="400" %}</td>
    <td>{% include milestone2/calibration_model2A.html width="400" %}</td>
  </tr>
</table>

- *Figure 4.2. - The four figures for model-2A. The model's AUC on the validation set is ~0.735.*

This is a better performance compared to the baseline model. Among the 10% of shots the model considered most dangerous, 25% were actual goals, an improvement compared to the baseline model (bottom left). Similarly, the top 20% of shots predicted by XGBoost account for 45% of all goals, while the top 40% account for 75% of all goals, a slight improvement over the baseline model. (top right). The calibration curve is also improved.
0.237



**Model 2B**
For model2B we started from the previous best parameters for (2A), and adjusted the grid to see if there is any change. We run a single cross-validation that gave us the best configuration as (max_depth=5, eta=0.75, min_child_weight=5, subsample=1.0, colsample_bytree=0.9) with an AUC of 0.739 (143 rounds) which we kept for training (Figure 4.3).\
See the relevant WB log entry [here](https://wandb.ai/IFT6758_team4/milestone_2/runs/9whkzrdq?nw=nwuserantonioslagarias).



<table>
  <tr>
    <td>{% include milestone2/roc_model2B.html width="400" %}</td>
    <td>{% include milestone2/percentile_model2B.html width="400" %}</td>
  </tr>
  <tr>
    <td>{% include milestone2/cumulative_model2B.html width="400" %}</td>
    <td>{% include milestone2/calibration_model2B.html width="400" %}</td>
  </tr>
</table>


- *Figure 4.3. - The four figures for model-2B. The model's AUC on the validation set is ~0.743*

Model 2B’s performance differs slightly from that of Model 2A. Its overall AUC is higher, however, among the 10% of shots the model identified as most dangerous, 23.7% were actual goals (compared to 25% for Model 2A). Similarly, the cumulative curve shows a slight improvement, while the calibration curve performs slightly worse. Therefore, we decided to register both models for testing in Step 6.




## Question 4.3: Optimised XGBoost classifier

**Handling Missing Values**\
To train an optimised Model-3, we need to examine the features to identify those most relevant for predicting goals. We begin with two filtering methods based on information theory, **information** gain and **mutual information**. For this step, the two categorical features (*shot type* and *last event type*) are excluded. Missing values in continuous numerical features are filled with their mean value, while missing values in the single boolean feature *rebound* are replaced with the most frequent value.

To also assess the relevance of categorical features, after selecting an initial feature set through information-theory filtering, we perform both **forward** and **backward wrapper** searches. These tests evaluate how individual features affect the performance of a base model, allowing us to determine the optimal set of features.


**Feature Selection: Filtering**\
We first uses information theory to better understand how each feature contributes to information gain. Figure 4.4 presents the four most significant features (left) and the four least significant ones (right).

| features (best)         | i_score  |               | features (worst)         | i_score  |
|--------------------------|-----------|-------------|--------------------------|-----------|
| distance_from_net        | 0.024107  |             | last_event_x             | 0.003538  |
| rebound                  | 0.023004  |             | time_since_last_event    | 0.002954  |
| period                   | 0.021879  |             | period_time_seconds      | 0.002904  |
| y_coord                  | 0.019276  |             | angle_change             | 0.000104  |

- *Figure 4.4. - The best and worst features in terms of information gain.*

\
However, we observe that some significant features appear redundant, such as *distance* and *y_coord*. Therefore, we proceed by testing all possible pairs of features to measure their mutual information. Figure 4.5 shows the pairs with the highest mutual information values.

| combination                                | mi       |   | combination                                | mi       |
|--------------------------------------------|-----------|---|--------------------------------------------|-----------|
| distance_from_net & shot_angle             | 7.019193  |   | y_coord & shot_angle                       | 2.818636  |
| last_event_distance & event_speed          | 4.472923  |   | time_since_last_event & event_speed        | 1.718200  |
| x_coord & distance_from_net                | 4.427126  |   | rebound & angle_change                     | 0.654856  |


- *Figure 4.5. - The pairs sharing the most mutual information.*

\
Based on these observations, we can separate the most relevant features from the redundant and irrelevant ones. However, the distinction is not always clear, so we proceed with a forward wrapper method to test whether the candidate features, as well as the categorical features previously excluded from filtering, improve the performance of the model trained on the relevant features (see Figure 4.6).


| relevance              | features                                                                 |
|------------------------|---------------------------------------------------------------------------|
| relevant features     | distance_from_net / rebound / period / last_event_distance              |
| irrelevant features   | angle_change / time_since_last_event / period_time_seconds / last_event_x |
| redundant features*    | y_coord / x_coord                                                       |
| candidate features     | shot_angle / event_speed / shot_type / last_event_type                  |

- *Figure 4.6. - The best and worst features in terms of information gain.*


\
**Feature Selection: Forward and Backward Wrapper**\
Our base model uses four features. For each feature added from the candidate set, we check whether the model’s performance improves. Based on these results, we select a total seven "final" features whose combinatinon gives the best results: distance_from_net, rebound, period, last_event_distance, shot_angle, shot_type, and period_time_seconds.

Finally, we run a single backward wrapper starting with all 14 features used in model 2B, removing one feature each time. We observe that removing any of the “final” features significantly worsens the model’s performance, while removing any other feature has only a minimal impact. We therefore proceed to train model 3 using the seven "final" features.



**Model-3**

For model 3, we started from the previous best parameters (from model 2B) and refined the grid. We ran two rounds of cross-validation, adjusting the values, which resulted in the configuration (max_depth=6, eta=0.06, min_child_weight=5, subsample=1, colsample_bytree=0.8) with an AUC of 0.741 after 155 rounds, which we kept for training (figure 4.7). See the relevant WB log entry [here](https://wandb.ai/IFT6758_team4/milestone_2/runs/6rf5t2xm?nw=nwuserantonioslagarias).


<table>
  <tr>
    <td>{% include milestone2/roc_model3.html width="400" %}</td>
    <td>{% include milestone2/percentile_model3.html width="400" %}</td>
  </tr>
  <tr>
    <td>{% include milestone2/cumulative_model3.html width="400" %}</td>
    <td>{% include milestone2/calibration_model3.html width="400" %}</td>
  </tr>
</table>


- *Figure 4.7. - The four figures for model-3. The model's AUC on the validation set is ~0.743*

Model-3’s overall performance is comparable to both Model-2A and Model-2B. Despite using fewer features, it maintains the best AUC achieved by Model 2B, and its cumulative and percentile curves closely resemble those of the best Model-2A. Although the calibration curve is not ideal, its fluctuations are similar to those observed in Model 2B.

---

# **Q6 — Give it your best shot!**

In Q6, our goal was to build the strongest **expected-goals** model we could on the engineered dataset, prioritizing a fair comparison across methods rather than over-tuning one approach. We evaluated several learners (Logistic/Ridge, RandomForest, HistGradientBoosting, CatBoost), plus stacking/blending, n-seed ensembling, and simple mixture-of-experts splits, all on a fixed **80/20 stratified split (seed 42)**. To keep results comparable, we locked a **17-feature** set derived from distance/angle and last-event context and reported metrics on the **validation** set only. The final choice is **CatBoost** with **3-fold OOF Platt calibration**: it provides the best single-model ranking and, crucially, more reliable probabilities for xG. The four figures below (ROC, goal-rate vs percentile, cumulative goals vs percentile, and reliability) summarize the validation performance; links to the tracked experiments are included alongside the plots.

## Setup

* **data:** `advanced_train.csv` (team FE upstream; we add a handful of derived fields).
* **target:** `is_goal` (1/0).
* **split:** stratified **80/20**, seed **42**. Calibration trains on **train-only** OOF; all plots/metrics are on **validation**.

## Features (locked 17)

* **base (7):** `distance_from_net, rebound, period, last_event_distance, shot_angle, shot_type, period_time_seconds`
* **context (4):** `time_since_last_event, angle_change, event_speed, last_event_type`
* **derived (6):** `log_distance, abs_angle, cos_angle, dist_x_abs_angle, rush(Δt≤2s), big_turn(|Δangle|≥30°)`
* **categorical:** `shot_type, last_event_type, period`

## Final model choice

**CatBoost** (depth=8, lr=0.05, l2=3, 800 iters, early-stop 50, class-balanced) + **3-fold OOF Platt calibration**.
Calibration is trained on train-fold OOF scores and applied to validation probabilities (no leakage).

**validation (seed-42):**

* **catboost (calibrated):** **ROC-AUC ≈ 0.858**, **PR-AUC ≈ 0.525**
* **hgb comparator:** ROC-AUC ≈ 0.856, PR-AUC ≈ 0.525
* **random:** ROC-AUC ≈ 0.498

**why this model?** it edges out alternatives on ranking and—after Platt—delivers cleaner probability calibration, which matters directly for xG use.

---

## Question 6.1:

## Methods & Ablations

| Approach                        | Key settings                                  |   Val ROC-AUC |    Val PR-AUC | Outcome / Notes                                                                              |
| ------------------------------- | --------------------------------------------- | ------------: | ------------: | -------------------------------------------------------------------------------------------- |
| **CatBoost (calibrated)**       | depth=8, lr=0.05, l2=3, 800; **OOF Platt**    |    **0.8579** |    **0.5251** | **Final model**; AUC unchanged vs raw, **better reliability** (Brier≈0.0629, LogLoss≈0.2222) |
| CatBoost (raw)                  | same, no calibration                          |        0.8579 |        0.5251 | Matches calibrated AUC; kept to show calibration helps probability quality                   |
| **HistGradientBoosting (+OHE)** | max_leaf_nodes=63, lr=0.08, min_leaf=50       |        0.8561 |        0.5254 | Strong comparator, essentially tied on PR-AUC                                                |
| **RF n-seed ensemble (5)**      | 300 trees, balanced; seeds 42/101/202/303/404 |        0.8428 |        0.5042 | Slower, clearly below CatBoost/HGB                                                           |
| Ridge / Logistic                | regularized linear                            | ~0.71 (prior) | ~0.20 (prior) | Not re-run post-fix; underfits on this task                                                  |
| Blends / Stacking               | Cat+HGB(+RF); meta-LR                         |    ~0.85 |    ≈ CatBoost | No material gain; omitted from final curves                                                  |
| Mixture-of-Experts              | rebound / distance×rush splits                |    ~0.83 |    < CatBoost | Data fragmentation + overlap → worse                                                         |

**Interpretation.** After the dataset fix, separability jumps to **AUC ≈ 0.86 / PR-AUC ≈ 0.525** for the top two models (CatBoost, HGB). **Calibration** leaves AUC unchanged—as expected—but improves probability quality (lower Brier/LogLoss), which matters for xG usage. Random Forest still lags, and added complexity (stacking/blends/MoE) doesn’t beat a single, calibrated CatBoost, so we ship the **calibrated CatBoost** as the final model.


## Required figures

{% include milestone2/q6_fig_roc.html width="600" %}

* **roc:** both models are strong and nearly overlapping; **catboost (calibrated)** is a hair above **hgb** across most fpr, matching the legend (~0.858 vs ~0.856). The very steep rise near the origin shows lots of easy positives; random is the 45° line.

{% include milestone2/q6_fig_goalrate.html width="600" %}

* **goal-rate vs percentile:** a clean monotonic drop. The top few percentiles have **very high goal rates** (spike at the far right), then decay smoothly. Catboost tracks or slightly beats hgb over most high-percentile bins; random is flat ~constant.

{% include milestone2/q6_fig_cum_goals.html width="600" %}

* **cumulative goals:** the highest-scored shots account for a **large majority of goals**; catboost and hgb curves almost overlap with catboost just ahead most of the way. Random is much closer to a straight line.

{% include milestone2/q6_fig_calibration.html width="600" %}

* **reliability:** both models are **reasonably calibrated** in the mid-range. Catboost is a bit **under-confident** at low–mid probabilities and then catches up; hgb hugs the diagonal longer but becomes **over-confident** in the highest bin (points above the line). Either way, calibrated catboost gives the tidiest probs overall.

---

## Question 6.2:
### W&B experiment links

- **Final model (CatBoost, calibrated) — run:** *[q6_catboost_final_calibrated](https://wandb.ai/IFT6758_team4/milestone_2/runs/kggfjh41?nw=nwuseraftabgazali003)*
- **Final model — artifact:** *[q6_catboost_final_calibrated:v2](https://wandb.ai/IFT6758_team4/milestone_2/artifacts/model/q6_catboost_final_calibrated/v2)*
- **HGB comparator — run:** *[q6_final_hgb_locked17](https://wandb.ai/IFT6758_team4/milestone_2/runs/rtzdqzn9?nw=nwuseraftabgazali003)*
- **CatBoost (raw) — run:** *[q6_catboost_raw_locked17](https://wandb.ai/IFT6758_team4/milestone_2/runs/jbxjv0j7?nw=nwuseraftabgazali003)*
- **nseed Random Forest - run:** *[q6_nseed_RF_locked17](https://wandb.ai/IFT6758_team4/milestone_2/runs/jamz7upp?nw=nwuseraftabgazali003)*

---

# **Q7 — Evaluate on test set!**

## Question 7.1:
## Question 7.2:


- *Figure 6.1 -*

---

# Sources / references


