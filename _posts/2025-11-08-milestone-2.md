---
layout: post
title: Milestone 2
---
## Introduction: How to Predict a Goal? 

Following the NHL data exploration of *Milestone 1*, we now dive deeper into the task of estimating the **quality of shots**. For this, we calculate the likelihood of  shot resulting in a goal (**Expected Goals**) based on available features, e.g. *shot type*, *distance*, *angle*, etc. This exercise consists of six steps:

- Step 1 introduces a first round of **feature engineering (I)** to prepare the dataset for linear regression models.  
- Step 2 runs initial tests with **linear regression**.  
- Step 3 presents a second round of **feature engineering (II)** to create advanced features for future models.  
- Step 4 tries different **hyperparameter tuning** and **feature selection** methods to determine the best parameters and features, and trains three **XGBoost** models (see Figures 4.1…). 
- Step 5 proposes the **best model** based on previous experiments.  
- Step 6 presents the **test results**.

All experiments and the most relevant models are logged on our [Weights & Biases page](https://wandb.ai/IFT6758_team4/milestone_2) (WB).



---


# **Step 1: Feature Engineering I**
## Question 1.1 : 
## Question 1.2 :
## Question 1.3 : 

<!-- ![Figure 1.1](assets/images/().png) -->

- *Figure 1.1 - *

---
# **Step 2: Baseline Models**
we trained three baseline **logistic regressions** on **distance only**, **angle only**, and **distance+angle**, plus a **random-uniform** baseline. evaluation used **validation probabilities** only.

**setup.** features: **distance_from_net**, **shot_angle**; target: **is_goal** (1 = goal). from `baseline_train.csv` we made a **stratified 80/20 split (seed 42)** and trained three default logistic regressions: (i) distance, (ii) angle, (iii) distance+angle. we also included a random-uniform baseline. missing values in the two features were imputed with the **median fitted on the training split**. the **test set was not used**.

**Evaluation.** because we care about **probabilities** (expected goals), all curves use **validation-set predicted probabilities for class 1 (goal)**:

1. **roc curve (+ auc)** with a **chance (45°)** line
2. **goal rate vs model percentile**
3. **cumulative % of goals vs percentile**
4. **reliability (calibration) diagram**

**Findings.**

- **distance+angle** ranks shots best; **distance** is next; **angle** alone is only slightly above chance; **random-uniform** tracks the chance line.
- high-probability percentiles have much higher **goal rates**, and **distance+angle** captures goals fastest in the cumulative plot.
- calibration isn’t perfect (typical for simple baselines): most predictions sit in a low-probability band and deviate from the diagonal, but the ranking remains useful.

## Question 2.1 : 
we split the data into training and validation using a stratified split so the goal rate is preserved. we then trained a default **logistic regression** on a single feature, **distance_from_net**, and evaluated **accuracy** on the validation set. the accuracy looks deceptively decent, but inspection of the predictions shows most probabilities are very small; with a 0.5 threshold, the model predicts almost all shots as “no goal.” because goals are relatively rare, a model can achieve high accuracy without actually identifying goals. **takeaway:** accuracy is a poor metric here; we should evaluate the **predicted probabilities** directly rather than hard 0/1 decisions.

## Question 2.2 :
Motivated by q3.1, we use `predict_proba` and evaluate **validation probabilities** for the positive class (goal). we generated four diagnostics:

- **roc curve + auc** (with a **chance (45°)** line): assesses ranking quality independent of a fixed threshold.
- **goal rate vs model percentile:** shows how the empirical goal rate changes as we move from the highest to the lowest predicted percentiles.
- **cumulative % of goals vs percentile:** shows how quickly each model captures the total pool of goals as we move down from the highest predicted shots.
- **reliability (calibration) diagram:** compares predicted probabilities to observed frequencies.

    these plots reveal that probability ranking is informative even when raw accuracy is misleading. (per the instructions, we defer showing the full, multi-model figures until the next subsection.)
## Question 2.3 : 
We repeated the same setup for **logistic regression on angle only**, and on **distance+angle**, and we added a **random-uniform** baseline (probabilities sampled from 0–1). we then produced the **same four figures** on the validation set, each containing **four curves** (distance, angle, distance+angle, random-uniform):

- **roc/auc:** **distance+angle** ranks shots best; **distance** is next; **angle** alone is weak; **random-uniform** tracks chance.
- **goal rate vs percentile:** the top predicted percentiles have the highest goal rates; curves order as distance+angle > distance > angle > random.
- **cumulative % of goals vs percentile:** distance+angle accumulates goals fastest, confirming stronger ranking.
- **reliability diagram:** probabilities are not perfectly calibrated (typical for simple baselines), but the ranking remains useful.
    
    **takeaway:** distance carries most of the signal; adding angle improves ranking further, while the random baseline provides the expected reference.
    
## Question 2.4

Next to the figures, we include links to the **three experiment runs** (distance, angle, distance+angle). For each of these runs, we also **saved the trained model as a W&B artifact** and tagged it clearly so it can be referenced later.

---

{% include milestone2/q2_fig_roc_auc.html width="600" %}

**Figure — ROC (+ AUC, ranking quality).** _lr-both_ ranks shots best (highest AUC), _lr-distance_ is next, and _lr-angle_ is only slightly above chance. The **random-uniform** curve hugs the **chance (45°)** line, as expected. **Takeaway:** distance carries most of the signal; adding angle improves ranking.

---

{% include milestone2/q2_fig_goalrate_vs_percentile.html width="600" %}

**Figure — Goal rate vs model percentile.** Goal rate is highest at the **top percentiles** and drops as we move lower. Curves order: **lr-both** > **lr-distance** > **lr-angle**; **random-uniform** is relatively flat. **Takeaway:** models meaningfully separate dangerous from low-quality shots.

---

{% include milestone2/q2_fig_cum_goals_vs_percentile.html width="600" %}

**Figure — Cumulative % of goals vs percentile.** **lr-both** rises fastest (captures goals earliest), **lr-distance** follows, **lr-angle** lags, and **random-uniform** increases nearly linearly. **Takeaway:** focusing on the top-probability shots covers a large fraction of goals.

---

{% include milestone2/q2_fig_reliability_diagram.html width="600" %}

**Figure — Reliability (calibration) diagram.** Most predictions lie in a **low-probability band**. Deviations from the diagonal indicate **imperfect calibration** (some under/over-confidence), which is typical for simple baselines. Ranking is still useful, as shown by the earlier plots.

---

- **Artifacts & runs.** The three models are saved as W&B **artifacts**, and each experiment run is linked below:

### Baseline Runs
- [lr-distance (run)](https://wandb.ai/IFT6758_team4/milestone_2/runs/kikm28qp?nw=nwuseraftabgazali003)
- [lr-angle (run)](https://wandb.ai/IFT6758_team4/milestone_2/runs/lcfix0ii?nw=nwuseraftabgazali003)
- [lr-both (run)](https://wandb.ai/IFT6758_team4/milestone_2/runs/88d0ye0e?nw=nwuseraftabgazali003)

### Saved Baseline Models (Artifacts)
- [lr-distance:v0](https://wandb.ai/IFT6758_team4/milestone_2/artifacts/model/lr-distance/v0)
- [lr-angle:v0](https://wandb.ai/IFT6758_team4/milestone_2/artifacts/model/lr-angle/v0)
- [lr-both:v0](https://wandb.ai/IFT6758_team4/milestone_2/artifacts/model/lr-both/v0)

**Note.** The **test set remains untouched** in this section; everything above uses the training/validation split only.


---
# **Step 3: Feature Engineering II**

## Question 3.1 : 
## Question 3.2 :
## Question 3.3 : 
## Question 3.4 :
## Question 3.5 :

<!-- ![Figure 3.1](assets/images/screenshot.png) -->

- *Figure 3.1 - *


---

# **Step 4: Advanced Models**
In this exercise, we test three approaches to training XGBoost classifiers for predicting the most dangerous shots. Our first baseline model (1) predicts the likelihood of a shot resulting in a goal based only on its *distance* and *shot angle*, without any hyperparameter fine-tuning. The second model (2A, using two input features) includes only numerical features, while the third model (2B) incorporates all 14 available features. For both, we apply hyperparameter fine-tuning with gread search and cross-validation. We then aim to identify the relevant, redundant, and weak features using information-theory methods. We compute information gain for each feature and mutual information for each pair to make a first selection, and then confirms the selection through both forward and backward wrappers. We end up selecting seven features as the most relevant and train our final model (3) on this set.

Since XGBoost can handle missing values natively, we ensure that all missing entries are formatted as np.nan and rely on the model’s internal mechanisms for handling them. For each model tested, we present the same four figures used in Question 3: ROC/AUC, Goal rate vs. probability percentile, Cumulative proportion of goals vs. probability percentile, and Reliability curve.


## Question 4.1: Baseline XGBoost classifier

**Model-1**


<table>
  <tr>
    <td>{% include milestone2/roc_model1.html width="400" %}</td>
    <td>{% include milestone2/percentile_model1.html width="400" %}</td>
  </tr>
  <tr>
    <td>{% include milestone2/cumulative_model1.html width="400" %}</td>
    <td>{% include milestone2/calibration_model1.html width="400" %}</td>
  </tr>
</table>


- *Figure 4.1. - The four figures for the baseline model (Model-1). The ROC curve shows an AUC of ~0.71 for the validation set. This indicates that the model can distinguish to some degree higher-quality shots from low-quality ones since it performs better than a random guess would (AUC=0.5).*

The percentile curve (top right) behaves as expected as the actual goal rate increases for shots in bins with higher predicted probabilities. For example, among the 10% of shots the model identified as most dangerous, 22.5% were actual goals. Similarly, the top 20% of shots by predicted xG account for 40% of all goals, and the top 40% account for 75% of all goals (bottom left). However, the calibration curve (bottom right) indicates that shots the model predicted as more likely to score actually scored less often than those with lower predicted probabilities.\
See the relevant WB log entry [here](https://wandb.ai/IFT6758_team4/milestone_2/runs/3j35f7mq?nw=nwuserantonioslagarias).

## Question 4.2: Full XGBoost classifier

XGBoost can handle both numerical and categorical features. To assess its performance on datasets containing a mix of feature types — such as our *advanced_data* — we test two models. First, we exclude the two categorical features, *shot type* and *previous event type* (model 2A, with 12 features). Then, we include them in model 2B, which has access to all 14 features. For both models, we optimize the hyperparameters using the same grid search approach with cross-validation (CV).

**Model-2A**

For model-2A, we ran cross-validation three times, adjusting the grid, optimizing for AUC. First, a 3-fold CV gave us the best configuration as (max_depth=5, eta=0.1, min_child_weight=5, subsample=1.0, colsample_bytree=0.8) with an AUC of 0.733 (73 rounds). A second 5-fold CV gave us (max_depth=5, eta=0.05, min_child_weight=1, subsample=1.0, colsample_bytree=0.8) with an AUC of 0.7334 (99 rounds). The third 5-fold CV produced the same configuration (max_depth=5, eta=0.05, min_child_weight=1, subsample=1.0, colsample_bytree=0.8) with an AUC of 0.7336 (122 rounds), which we kept for training (see figure 4.2).\
See the relevant WB log entry [here](https://wandb.ai/IFT6758_team4/milestone_2/runs/escltr3d/artifacts?nw=nwuserantonioslagarias).


<table>
  <tr>
    <td>{% include milestone2/roc_model2A.html width="400" %}</td>
    <td>{% include milestone2/percentile_model2A.html width="400" %}</td>
  </tr>
  <tr>
    <td>{% include milestone2/cumulative_model2A.html width="400" %}</td>
    <td>{% include milestone2/calibration_model2A.html width="400" %}</td>
  </tr>
</table>

- *Figure 4.2. - The four figures for model-2A. The model's AUC on the validation set is ~0.735.*

This is a better performance compared to the baseline model. Among the 10% of shots the model considered most dangerous, 25% were actual goals, an improvement compared to the baseline model (bottom left). Similarly, the top 20% of shots predicted by XGBoost account for 45% of all goals, while the top 40% account for 75% of all goals, a slight improvement over the baseline model. (top right). The calibration curve is also improved.
0.237



**Model 2B**
For model2B we started from the previous best parameters for (2A), and adjusted the grid to see if there is any change. We run a single cross-validation that gave us the best configuration as (max_depth=5, eta=0.75, min_child_weight=5, subsample=1.0, colsample_bytree=0.9) with an AUC of 0.739 (143 rounds) which we kept for training (Figure 4.3).\
See the relevant WB log entry [here](https://wandb.ai/IFT6758_team4/milestone_2/runs/9whkzrdq?nw=nwuserantonioslagarias).



<table>
  <tr>
    <td>{% include milestone2/roc_model2B.html width="400" %}</td>
    <td>{% include milestone2/percentile_model2B.html width="400" %}</td>
  </tr>
  <tr>
    <td>{% include milestone2/cumulative_model2B.html width="400" %}</td>
    <td>{% include milestone2/calibration_model2B.html width="400" %}</td>
  </tr>
</table>


- *Figure 4.3. - The four figures for model-2B. The model's AUC on the validation set is ~0.743*

Model 2B’s performance differs slightly from that of Model 2A. Its overall AUC is higher, however, among the 10% of shots the model identified as most dangerous, 23.7% were actual goals (compared to 25% for Model 2A). Similarly, the cumulative curve shows a slight improvement, while the calibration curve performs slightly worse. Therefore, we decided to register both models for testing in Step 6.




## Question 4.3: Optimised XGBoost classifier

**Handling Missing Values**\
To train an optimised Model-3, we need to examine the features to identify those most relevant for predicting goals. We begin with two filtering methods based on information theory, **information** gain and **mutual information**. For this step, the two categorical features (*shot type* and *last event type*) are excluded. Missing values in continuous numerical features are filled with their mean value, while missing values in the single boolean feature *rebound* are replaced with the most frequent value.

To also assess the relevance of categorical features, after selecting an initial feature set through information-theory filtering, we perform both **forward** and **backward wrapper** searches. These tests evaluate how individual features affect the performance of a base model, allowing us to determine the optimal set of features.


**Feature Selection: Filtering**\
We first uses information theory to better understand how each feature contributes to information gain. Figure 4.4 presents the four most significant features (left) and the four least significant ones (right).

| features (best)         | i_score  |               | features (worst)         | i_score  |
|--------------------------|-----------|-------------|--------------------------|-----------|
| distance_from_net        | 0.024107  |             | last_event_x             | 0.003538  |
| rebound                  | 0.023004  |             | time_since_last_event    | 0.002954  |
| period                   | 0.021879  |             | period_time_seconds      | 0.002904  |
| y_coord                  | 0.019276  |             | angle_change             | 0.000104  |

- *Figure 4.4. - The best and worst features in terms of information gain.*

\
However, we observe that some significant features appear redundant, such as *distance* and *y_coord*. Therefore, we proceed by testing all possible pairs of features to measure their mutual information. Figure 4.5 shows the pairs with the highest mutual information values.

| combination                                | mi       |   | combination                                | mi       |
|--------------------------------------------|-----------|---|--------------------------------------------|-----------|
| distance_from_net & shot_angle             | 7.019193  |   | y_coord & shot_angle                       | 2.818636  |
| last_event_distance & event_speed          | 4.472923  |   | time_since_last_event & event_speed        | 1.718200  |
| x_coord & distance_from_net                | 4.427126  |   | rebound & angle_change                     | 0.654856  |


- *Figure 4.5. - The pairs sharing the most mutual information.*

\
Based on these observations, we can separate the most relevant features from the redundant and irrelevant ones. However, the distinction is not always clear, so we proceed with a forward wrapper method to test whether the candidate features, as well as the categorical features previously excluded from filtering, improve the performance of the model trained on the relevant features (see Figure 4.6).


| relevance              | features                                                                 |
|------------------------|---------------------------------------------------------------------------|
| relevant features     | distance_from_net / rebound / period / last_event_distance              |
| irrelevant features   | angle_change / time_since_last_event / period_time_seconds / last_event_x |
| redundant features*    | y_coord / x_coord                                                       |
| candidate features     | shot_angle / event_speed / shot_type / last_event_type                  |

- *Figure 4.6. - The best and worst features in terms of information gain.*


\
**Feature Selection: Forward and Backward Wrapper**\
Our base model uses four features. For each feature added from the candidate set, we check whether the model’s performance improves. Based on these results, we select a total seven "final" features whose combinatinon gives the best results: distance_from_net, rebound, period, last_event_distance, shot_angle, shot_type, and period_time_seconds. 

Finally, we run a single backward wrapper starting with all 14 features used in model 2B, removing one feature each time. We observe that removing any of the “final” features significantly worsens the model’s performance, while removing any other feature has only a minimal impact. We therefore proceed to train model 3 using the seven "final" features.



**Model-3**

For model 3, we started from the previous best parameters (from model 2B) and refined the grid. We ran two rounds of cross-validation, adjusting the values, which resulted in the configuration (max_depth=6, eta=0.06, min_child_weight=5, subsample=1, colsample_bytree=0.8) with an AUC of 0.741 after 155 rounds, which we kept for training (figure 4.7). See the relevant WB log entry [here](https://wandb.ai/IFT6758_team4/milestone_2/runs/6rf5t2xm?nw=nwuserantonioslagarias).


<table>
  <tr>
    <td>{% include milestone2/roc_model3.html width="400" %}</td>
    <td>{% include milestone2/percentile_model3.html width="400" %}</td>
  </tr>
  <tr>
    <td>{% include milestone2/cumulative_model3.html width="400" %}</td>
    <td>{% include milestone2/calibration_model3.html width="400" %}</td>
  </tr>
</table>


- *Figure 4.7. - The four figures for model-3. The model's AUC on the validation set is ~0.743*

Model-3’s overall performance is comparable to both Model-2A and Model-2B. Despite using fewer features, it maintains the best AUC achieved by Model 2B, and its cumulative and percentile curves closely resemble those of the best Model-2A. Although the calibration curve is not ideal, its fluctuations are similar to those observed in Model 2B.

---

# **Q6 — Give it your best shot!**

In Q6, our goal was to build the strongest **expected-goals** model we could on the engineered dataset, prioritizing a fair comparison across methods rather than over-tuning one approach. We evaluated several learners (Logistic/Ridge, RandomForest, HistGradientBoosting, CatBoost), plus stacking/blending, n-seed ensembling, and simple mixture-of-experts splits, all on a fixed **80/20 stratified split (seed 42)**. To keep results comparable, we locked a **17-feature** set derived from distance/angle and last-event context and reported metrics on the **validation** set only. The final choice is **CatBoost** with **3-fold OOF Platt calibration**: it provides the best single-model ranking and, crucially, more reliable probabilities for xG. The four figures below (ROC, goal-rate vs percentile, cumulative goals vs percentile, and reliability) summarize the validation performance; links to the tracked experiments are included alongside the plots.

### Setup

- **Data:** `advanced_train.csv` (team FE upstream; we add a few simple derived fields only).
- **Target:** `is_goal` (1=goal).
- **Split:** stratified **80/20**, seed **42**. All calibration is fit on **train only**; all figures/metrics are on **validation**.

### Features (locked 17)

- **Base (7):** `distance_from_net, rebound, period, last_event_distance, shot_angle, shot_type, period_time_seconds`
- **Context (4):** `time_since_last_event, angle_change, event_speed, last_event_type`
- **Derived (6):** `log_distance, abs_angle, cos_angle, dist_x_abs_angle, rush(Δt≤2s), big_turn(|Δangle|≥30°)`
- **Categorical:** `shot_type, last_event_type, period`

## Final Model choice

**CatBoost** (depth=8, lr=0.05, l2=3, 800 iters, early-stop 50, class-balanced) + **3-fold OOF Platt (sigmoid) calibration**.

Calibration is trained on train-fold OOF scores and applied to validation probabilities (no leakage).

**Validation (seed-42 split):**

- **CatBoost (calibrated)**: **ROC-AUC ≈ 0.751**, **PR-AUC ≈ 0.236**
- **HGB comparator**: ROC-AUC ≈ 0.748, PR-AUC ≈ 0.236
- **Random**: ROC-AUC ≈ 0.498

Why this model? It’s the best single model we found and, after Platt scaling, produces **better-calibrated probabilities** (which matters for xG) without sacrificing AUC.

---

## Question 6.1:

## Methods & Abalations

| Approach | Key settings | Val ROC-AUC | Val PR-AUC | Outcome / Notes |
| --- | --- | --- | --- | --- |
| CatBoost (calibrated) | depth=8, lr=0.05, l2=3, 800; OOF Platt | ≈0.751 | ≈0.236 | Final model (better reliability; artifact logged) |
| CatBoost (raw) | same, no calibration | ≈0.750–0.751 | ≈0.233–0.236 | Similar AUC; worse calibration → included to show benefit |
| HGB (+OHE) | max_leaf_nodes≈63, lr≈0.08, min_leaf=50 | ≈0.748 | ≈0.236 | Strong comparator; on plots |
| RF n-seed ensemble | 5 seeds, 300 trees, balanced | ≈0.75–0.752 | ≈0.235 | Tiny/no gain; |
| Ridge / Logistic | regularized linear | ≈0.712 | ≈0.198 | Underfits |
| Blends / Stacking | Cat+HGB(+RF); meta-LR | ≤≈0.7516–0.7518 | ≈0.235–0.237 | Noise-level gains; omitted from final curves |
| Mixture-of-Experts | rebound / distance×rush splits | 0.60–0.74 | 0.17–0.24 | Fragmentation + overlap → worse |

**Interpretation** With location/angle/timing and last-event context, many goal/non-goal shots occupy overlapping regions (e.g., close-bad-angle vs far-good-angle; rush vs settled). That **caps separability near ~0.75 AUC**. For xG, **calibrated probabilities** are more important than tiny AUC gains, which is why we ship the calibrated CatBoost.

### Required figures
---

{% include milestone2/q6_fig_roc.html width="600" %}

- **ROC**: CatBoost (calibrated) edges HGB and is far above random; models plateau around the same shape, with CatBoost slightly better ranking overall.

{% include milestone2/q6_fig_goalrate.html width="600" %}

- **Goal-rate vs percentile:** clean monotonic drop; top-scored shots have much higher goal rates. CatBoost is a touch higher than HGB across most high-percentile bins.

{% include milestone2/q6_fig_cum_goals.html width="600" %}

- **Cumulative goals:** CatBoost and HGB capture a disproportionate share of goals in the highest percentiles (well above the random diagonal); the two curves are nearly overlapping, with CatBoost very slightly ahead.

{% include milestone2/q6_fig_calibration.html width="600" %}

- **Reliability:** Random is flat (uncalibrated). CatBoost after Platt is **closer to the diagonal** in the mid-probability region; both models **underpredict at the highest bins** (points sit above the line), so probabilities are usable but not perfect at extremes.

---

## Question 6.2:
### W&B experiment links

- **Final model (CatBoost, calibrated) — run:** *[q6_catboost_final_calibrated](https://wandb.ai/IFT6758_team4/milestone_2/runs/fk8z122k?nw=nwuseraftabgazali003)*
- **Final model — artifact:** *[q6_catboost_final_calibrated:v1](https://wandb.ai/IFT6758_team4/milestone_2/artifacts/model/q6_catboost_final_calibrated/v1)*
- **HGB comparator — run:** *[q6_final_hgb_locked17](https://wandb.ai/IFT6758_team4/milestone_2/runs/2lj1y7y7?nw=nwuseraftabgazali003)*
- **CatBoost (raw) — run:** *[q6_catboost_raw_locked17](https://wandb.ai/IFT6758_team4/milestone_2/runs/8z95fczn?nw=nwuseraftabgazali003)*
- **nseed Random Forest - run:** *[q6_nseed_RF_locked17](https://wandb.ai/IFT6758_team4/milestone_2/runs/jaez1wwe?nw=nwuseraftabgazali003)*

# **Q7 — Evaluate on test set!**

## Question 7.1:
## Question 7.2:


- *Figure 6.1 -*

---

# Sources / references


