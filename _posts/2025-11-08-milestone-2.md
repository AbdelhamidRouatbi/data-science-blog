---
layout: post
title: Milestone 2
---
## Introduction: How to Predict a Goal?

Following the NHL data exploration of *Milestone 1*, we now dive deeper into the task of estimating the **quality of shots**. For this, we define the quality of a shot by the likelihood of it becoming a goal (**Expected Goals**) based on available features, e.g. *shot type*, *distance*, *angle*, etc. This exercise consists of six steps:

- Step 1 introduces a first round of **feature engineering** to prepare the dataset for logistic regression models.
- Step 2 runs initial tests with **logistic regression**.
- Step 3 presents a second round of **feature engineering** to create advanced features for future models.
- Step 4 tries different **hyperparameter tuning** and **feature selection** methods to determine the best parameters and features, and trains three **XGBoost** models.
- Step 5 presents **diverse models** in order to find the best candidates.
- Step 6 presents the **test results**.

All experiments and the most relevant models are logged on our [Weights & Biases page](https://wandb.ai/IFT6758_team4/milestone_2) (WB).



---


# **Step 1: Feature Engineering I**
The first steps consists of preparing a training set for simple baseline models. To this end, we only use shot distance, shot angle, and whether the net is empty or not. We also perform a brief analysis on this data.

## Question 1 ##

<figure>
  {% include milestone2/distance_histogram.html %}
  <figcaption>Figure 1.1: Shot distance distribution histogram.</figcaption>
</figure>
Figure 1.1 is a histogram showing how shot frequency varies with distance from the net. From this figure, we understand that most shots are taken close to the net. Frequency drops sharply as distance goes beyond 60 feet. The red region to the left indicates that goals occur mostly at short ranges, which means that proximity to the net is strongly correlated with scoring likelihood.

<figure>
  {% include milestone2/angle_histogram.html %}
  <figcaption>Figure 1.2: Shot angle distribution histogram.</figcaption>
</figure>
Figure 1.2 is a histogram showing how shot frequency varies by shot angle. Most shots occur at narrow angles below 40°, where players easily face the net. As the angle widens, both total shots and goals decline sharply, indicating that shots from sharper angles are less frequent and less likely to result in goals.

<figure>
  {% include milestone2/distance_angle_histogram.html %}
  <figcaption>Figure 1.3: Shot distance-angle histogram.</figcaption>
</figure>
Figure 1.3 is a 2D histogram that shows the joint distribution of shot distance and shot angle. Most shots cluster at short distances (under 50 feet) and moderate angles (below 40°). This means that players typically shoot from close range and relatively central positions. The density quickly decreases for longer distances and wider angles, indicating that shots far from the net or from extreme angles are rare.

## Question 2 ##
<figure>
  {% include milestone2/goalrate_distance.html %}
  <figcaption>Figure 1.4: Goal rate by shot distance.</figcaption>
</figure>
Figure 1.4 shows how the goal rate as a function of shot distance. The goal rate is highest very close to the net, then drops charply as distance increases, reaching a low and almost stable level beyond 50 units. This confirms that shots taken near the net have a much higher change of scoring, and that long-range attempts rarely result in goals. The small rise on the right of the plot likely comes from empty-net long-distance goals.

<figure>
  {% include milestone2/goalrate_angle.html %}
  <figcaption>Figure 1.5: Goal rate by shot angle.</figcaption>
</figure>
discuss
Figure 1.5 shows the goal rate as a function of shot angle. Goal rate decreases steadily as the angle widens. Shots taken from more central positions are far more likely to score, and those from the sides of the net have a significantly lower goal probability. This shows that head-on shots are the most effective.

## Question 3 ##
<figure>
  {% include milestone2/goals_distance.html %}
  <figcaption>Figure 1.6: Goals distance histogram.</figcaption>
</figure>
Figure 1.6 is a histogram of the distribution of goal distances from the net. We see that almost all non-empty net goals occur within 30-40 feet of the net. If we zoom on the right side of the figure (distance > 100), we notice that the empty-net long-distance goals dominate the non-empty net long-distance goals. This confirms the domain knowledge that it's extremely rare to score a non-empty net goal from one's own defensive zone.

### **Investigating events** ###

Here, we investigate three goal events to check whether their features are correct or not. The goals in question can be visualized below.

**Goal 1:** We investigate the event with the highest distance non-empty net goal. The shot was taken at a distance of 187 feet, by Derek MacKenzie. We see that the shooting player is in reality right next to the net. It is very clear that the distance is wrong. This means one of the features used to calculate distance from net is wrong.

**Goal 2:** We also investigate the highest distance goal on an empty net. From the visualization, we confirm that it is indeed a long-distance goal from behind the player's own team's net. The puck bounces off boards and heads straight into the opponent's net. The play might have been intented as a pass to a teammate. Otherwise, it demonstrates remarkable precision.

**Goal 3:** Lastly, we check a goal that has an angle of 90°. For this, we picked a goal by Brian Dumoulin against Columbus Blue Jackets at the second period. The visualization shows a very impressive shot from behind the net. The puck bounces off a player to reach the back of the net and score a goal. We also confirmed other features available on <a href="https://www.nhl.com/gamecenter/cbj-vs-pit/2017/04/04/2016021181">NHL gamecenter</a>.

{% include milestone2/goals_gifs.html %}

---
# **Step 2: Baseline Models**

In this step, we trained three baseline logistic regression models. One on distance only, one on angle only, and one on distance and angle. We also performed a random-uniform baseline. Evaluation used validation probabilities only.

### **Setup**
- Features: `distance_from_net` and `shot_angle`
- Target: `is_goal` (1 = goal)

From `baseline_train.csv` we made a stratified 80/20 split (seed 42) and trained three default logistic regression models on different sets of features: *M1* (distance), *M2* (angle), and *M3* (distance and angle). We also included a random baseline *R*. Missing values in the two features were imputed with the median fitted on the training split. The test set was not used.

### **Evaluation**
Because we care about probabilities (expected goals), all curves use validation-set predicted probabilities for class 1 (goal):

1. ROC curve (+AUC) with a chance (45°) line
2. Goal rate vs model percentile
3. Cumulative % of goals vs percentile
4. Reliability (calibration) diagram

### **Findings**

* *M3* gives the best ranking (AUC ≈ 0.707). *M1* is close behind (AUC ≈ 0.692). *M2* is weak (AUC ≈ 0.560), and *R* (random baseline) sits at chance (AUC ≈ 0.496).
* High-probability percentiles have much higher goal rates; *M3* stays above the others across most of the curve, with *M1* next and *M2* trailing; *R* is nearly flat.
* In the cumulative plot, *M3* captures goals fastest, then *M1*, then *M2*; *R* increases almost linearly.
* Calibration isn’t perfect: most predictions lie in a low-probability band, and points deviate from the diagonal (some under/over-confidence), but the ranking remains useful.

---

## **Question 2.1**

We split the data into training and validation sets using a stratified split so that the goal rate is preserved.
We then trained a default logistic regression model on a single feature, `distance_from_net`, and evaluated accuracy on the validation set.

The accuracy looks deceptively decent, but inspection of the predictions shows that most probabilities are very small.
With a 0.5 threshold, the model predicts almost all shots as “no goal.”
Because goals are relatively rare, a model can achieve high accuracy without actually identifying goals.

**Takeaway:** Accuracy is a poor metric here; we should evaluate the predicted probabilities directly rather than hard 0/1 decisions.

---

## **Question 2.2**

Motivated by Q2.1, we use `predict_proba` and evaluate validation probabilities for the positive class (goal).
We generated four diagnostics: ROC/AUC, goal rate vs percentile, cumulative % of goals vs percentile, and a reliability diagram.

These results show that probability ranking is informative even when raw accuracy is misleading.

---

## **Question 2.3**

We repeated the setup for angle only and distance + angle, and added a random-uniform baseline.
We produced the same four figures on the validation set, each with four curves.

* ROC/AUC: *distance + angle (≈ 0.707)* > *distance (≈ 0.692)* >> *angle (≈ 0.560)* > *random (≈ 0.496)*
* Goal rate vs percentile: Top percentiles show the highest goal rates; ordering is *distance + angle* > *distance* > *angle* > *random* (random is nearly flat).
* Cumulative % of goals: *distance + angle* accumulates goals fastest across percentiles, confirming stronger ranking than single-feature models.
* Reliability: Models are not perfectly calibrated; most predictions cluster around low probabilities, with some bins showing under/over-confidence (especially at higher predicted values where sample sizes are smaller).

**Takeaway:** Distance carries most of the signal; adding angle improves ranking further; random behaves as expected.

---

## **Question 2.4**

{% include milestone2/q2_fig_roc_auc.html width="600" %}

**Figure 2.1 — ROC (+AUC, ranking quality).** *lr-both* (AUC ≈ 0.707) ranks shots best, *lr-distance* (AUC ≈ 0.692) follows, *lr-angle* (AUC ≈ 0.560) is only slightly above chance, and *random-uniform* (AUC ≈ 0.496) tracks the chance (45°) line.

---

{% include milestone2/q2_fig_goalrate_vs_percentile.html width="600" %}

**Figure 2.2 — Goal rate vs model percentile.** Goal rate peaks at the top percentiles and declines steadily.
Ordering across most of the curve is *lr-both* > *lr-distance* > *lr-angle*; *random-uniform* is relatively flat.

---

{% include milestone2/q2_fig_cum_goals_vs_percentile.html width="600" %}

**Figure 2.3 — Cumulative % of goals vs percentile.** *lr-both* rises fastest (captures goals earliest), *lr-distance* follows closely, *lr-angle* lags, and *random-uniform* increases nearly linearly.

---

{% include milestone2/q2_fig_reliability_diagram.html width="600" %}

**Figure 2.4 — Reliability (calibration) diagram.** Most predictions sit in a low-probability band; deviations from the diagonal indicate imperfect calibration (some under/over-confidence), which is typical for simple baselines.
Despite this, ranking remains useful, as confirmed by the ROC and percentile plots.

---

### **Artifacts and Runs**
The three models are saved as W&B artifacts, and each experiment run is linked below.

#### **Baseline Runs**
- [lr-distance (run)](https://wandb.ai/IFT6758_team4/milestone_2/runs/26ipd8pc?nw=nwuseraftabgazali003)
- [lr-angle (run)](https://wandb.ai/IFT6758_team4/milestone_2/runs/4qcqb2hw?nw=nwuseraftabgazali003)
- [lr-both (run)](https://wandb.ai/IFT6758_team4/milestone_2/runs/3g7slfrs?nw=nwuseraftabgazali003)

#### **Saved Baseline Models (Artifacts)**
- [lr-distance:v1](https://wandb.ai/IFT6758_team4/milestone_2/artifacts/model/lr-distance/v1)
- [lr-angle:v1](https://wandb.ai/IFT6758_team4/milestone_2/artifacts/model/lr-angle/v1)
- [lr-both:v1](https://wandb.ai/IFT6758_team4/milestone_2/artifacts/model/lr-both/v1)

---

**Note:** The test set remains untouched in this section; everything above uses the training/validation split only.

---
# **Step 3: Feature Engineering II**
In this step, we used the raw data to add more features to our dataset.

- **period_time_seconds** : Time elapsed in the current period, measured in seconds
- **period** : Current period of the game (1, 2, 3, or 4)
- **x_coord** : X-coordinate of the event on the rink
- **y_coord** : Y-coordinate of the event on the rink
- **distance_from_net** : Euclidean distance from the event location to the center of the net
- **shot_angle** : Angle between the shot location and the center of the goal
- **shot_type** : Type of shot taken (e.g., wrist, slap, snap, backhand)
- **empty_net** : Whether the goalie's net was empty during the play
- **last_event_type** : Type of the immediately preceding event
- **last_event_x** : X-coordinate of the previous event
- **last_event_y** : Y-coordinate of the previous event
- **time_since_last_event** : Time (in seconds) elapsed since the last recorded event
- **last_event_distance** : Distance between the current event and the previous event
- **rebound** : Whether the shot was taken shortly after a previous shot (by the same team).
- **angle_change** : Change in shot angle compared to the previous shot
- **event_speed** : Speed of play between the last event and the current one
- **friendly_player_count** : Number of players on the shooter's team on the rink
- **opponent_player_count** : Number of players on the opposing team on the rink
- **player_count_diff** : Difference between friendly and opponent player counts (on the rink)
- **time_since_powerplay** : Time elapsed since the start of the most recent power play
- **is_goal** : Target variable indicating whether the shot resulted in a goal

We created the features from all the questions (including bonus power-play features). We also added another feature, **player_count_diff**, which is the difference between **friendly_player_count** and **opponent_player_count**. The experiment for the Winnipeg vs Washington game dataframe can be viewed <a href="https://wandb.ai/IFT6758_team4/milestone_2/runs/rgzgn2vx?nw=nwuserabdelhamidrouatbi">here</a>.

---

# **Step 4: Advanced Models**
In this exercise, we test three approaches to training XGBoost classifiers for predicting the most dangerous shots. Our first baseline model (1) predicts the likelihood of a shot resulting in a goal based only on its *distance* and *shot angle*, without any hyperparameter fine-tuning. The second model (2A, using 14 input features) includes only numerical features (continuous or booulean), while the third model (2B) uses all 20 available features, ignoring question of redudancy. For both, we apply hyperparameter fine-tuning with gread search and cross-validation. We then aim to identify the relevant, redundant, and weak features using information-theory methods. We compute information gain for each feature and mutual information for each pair to make a first selection, and then confirms the selection through both forward and backward wrappers. We end up selecting nine features as the most relevant and train our final model (3) on this set.

Since XGBoost can handle missing values natively, we ensure that all missing entries are formatted as np.nan and rely on the model’s internal mechanisms for handling them. For each model tested, we present the same four figures used in Question 3:\
ROC/AUC\
Goal rate vs. probability percentile\
Cumulative proportion of goals vs. probability percentile\
Reliability curve.


## Question 4.1: Baseline XGBoost classifier

**Model-1**


<table>
  <tr>
    <td>{% include milestone2/roc_model1.html width="400" %}</td>
    <td>{% include milestone2/percentile_model1.html width="400" %}</td>
  </tr>
  <tr>
    <td>{% include milestone2/cumulative_model1.html width="400" %}</td>
    <td>{% include milestone2/calibration_model1.html width="400" %}</td>
  </tr>
</table>


- *Figure 4.1. - The four figures for the baseline model (Model-1). The ROC curve shows an AUC of ~0.715 for the validation set. This indicates that the model can distinguish to some degree higher-quality shots from low-quality.*

The percentile curve (top right) behaves as expected as the actual goal rate increases for shots in bins with higher predicted probabilities. For example, among the 10% of shots the model identified as most dangerous, ~22.5% were actual goals. Similarly, the top 20% of shots by predicted xG account for 40% of all goals, and the top 40% account for 70% of all goals (bottom left). The calibration curve (bottom right) shows that the model does not give high propabilities overal, yet its behavior appears to be correct, along the line of a perfect calibration. See the relevant WB log entry [here](https://wandb.ai/IFT6758_team4/milestone_2/runs/fzldzg63?nw=nwuserantonioslagarias).

## Question 4.2: Full XGBoost classifier

XGBoost can handle numerical, boolean and categorical features. To assess its performance on datasets containing a mix of feature types we test two models. First, we exclude the six categorical features, *period*, *shot_type*,  *last_event_type*,  *player_count_diff*,  *friendly_player_count*,  and  *opponent_player_count* (for now we are not interested in possibly redudant features, see question 4.3). Model 2A is thus trained with 14 features. Then, we include the categorical features in model 2B, which is trained on all 20 features. Since XGBoost is able to handle non-normalized features, we kept the original values for continuous variables.

For both models, we optimize the hyperparameters using the same grid search approach with cross-validation (CV).

**Model-2A**\
For model-2A, we ran cross-validation three times, adjusting the grid, optimizing for AUC. First, a 3-fold CV gave us the best configuration as (max_depth=5, eta=0.1, min_child_weight=5, subsample=1.0, colsample_bytree=0.8). A second 5-fold CV gave us (max_depth=5, eta=0.075, min_child_weight=5, subsample=0.9, colsample_bytree=0.9). The third 5-fold CV gave us a configuration of (max_depth=6, eta=0.05, min_child_weight=5, subsample=0.9, colsample_bytree=0.9) with an AUC of 0.82 (149 rounds), which we kept for training (see figure 4.2). See the relevant WB log entry [here](https://wandb.ai/IFT6758_team4/milestone_2/runs/hvvjqcq9?nw=nwuserantonioslagarias).


<table>
  <tr>
    <td>{% include milestone2/roc_model2A.html width="400" %}</td>
    <td>{% include milestone2/percentile_model2A.html width="400" %}</td>
  </tr>
  <tr>
    <td>{% include milestone2/cumulative_model2A.html width="400" %}</td>
    <td>{% include milestone2/calibration_model2A.html width="400" %}</td>
  </tr>
</table>

- *Figure 4.2. - The four figures for model-2A. The model's AUC on the validation set jumps to ~0.818.*

This is a better performance compared to the baseline model. Among the 10% of shots the model considered most dangerous, 40% were actual goals (bottom left). Similarly, the top 20% of shots predicted by XGBoost account for 60% of all goals, while the top 40% account for 80% of all goals, a significant improvement over the baseline model (top right). The calibration curve is also improved.


**Model 2B**\
For model2B we started from the previous best parameters for (2A), and adjusted the grid to see if there is any change. We run a single cross-validation that gave us the best configuration as (max_depth=6, eta=0.05, min_child_weight=1, subsample=1.0, colsample_bytree=0.8) with an AUC of 0.867 (149 rounds) which we kept for training (Figure 4.3). See the relevant WB log entry [here](https://wandb.ai/IFT6758_team4/milestone_2/runs/qltowz29?nw=nwuserantonioslagarias).



<table>
  <tr>
    <td>{% include milestone2/roc_model2B.html width="400" %}</td>
    <td>{% include milestone2/percentile_model2B.html width="400" %}</td>
  </tr>
  <tr>
    <td>{% include milestone2/cumulative_model2B.html width="400" %}</td>
    <td>{% include milestone2/calibration_model2B.html width="400" %}</td>
  </tr>
</table>


- *Figure 4.3. - The four figures for model-2B. The model's AUC on the validation set is ~0.859*

Model 2B’s performance is signficantly better compared to Model 2A. Its overall AUC is higher, among the 10% of shots the model identified as most dangerous, 44% were actual goals (compared to 40% for Model 2A). Similarly, the cumulative curve shows a signficant improvement as the top 20% account for 68% of all goals, while the calibration curve is slightly worse. Therefore, we decided to register model B for testing in Step 6.




## Question 4.3: Optimised XGBoost classifier

**Handling Missing Values**\
To train an optimised Model-3, we need to examine the features to identify those most relevant for predicting goals. We begin with two filtering methods based on information theory, **information gain** and **mutual information**. For this step, the six categorical features are excluded. Missing values in continuous numerical features are filled with their mean value, while missing values in boolean features are replaced with the most frequent value.

To also assess the relevance of categorical features, after selecting an initial feature set through information-theory filtering, we perform a **forward wrapper** search. These tests evaluate how individual features affect the performance of a base model, allowing us to determine the optimal set of features.


**Feature Selection: Filtering**\
We first uses information theory to better understand how each feature contributes to information gain. Figure 4.4 presents the five most significant features (left) and the five least significant ones (right).

| Feature (best)       | Score  |   | Feature (worst)       | MI Score  |
|----------------------------|-----------|---|---------------------------|-----------|
| last_event_distance        | 0.023960  |   | rebound                   | 0.004572  |
| distance_from_net          | 0.022452  |   | last_event_x              | 0.004564  |
| time_since_last_event      | 0.018821  |   | period_time_seconds       | 0.002767  |
| y_coord                    | 0.018155  |   | time_since_powerplay      | 0.001328  |
| x_coord                    | 0.016009  |   | angle_change              | 0.001109  |

- *Figure 4.4. - The best and worst features in terms of information gain.*

\
However, we observe that some significant features are related, such as distances and coordinates. Therefore, we proceed by testing all possible pairs of features to measure their mutual information. Figure 4.5 shows the pairs with the highest mutual information values.

| combination                                | mi       |   | combination                                | mi       |
|--------------------------------------------|-----------|---|--------------------------------------------|-----------|
| distance_from_net & shot_angle             | 5.827783  |   | y_coord & distance_from_net                | 2.786346  |
| x_coord & distance_from_net                | 3.511225  |   | y_coord & shot_angle                       | 2.581976  |
| x_coord & shot_angle                       | 3.135125  |   | last_event_x & last_event_distance         | 1.211732  |
| last_event_distance & event_speed          | 2.945407  |   | time_since_last_event & event_speed        | 1.207063  |


- *Figure 4.5. - The pairs sharing the most mutual information.*

\
Based on these observations, we can separate the most relevant features from the redundant and irrelevant ones. However, the distinction is not always clear, so we proceed with a forward wrapper method to test whether the candidate features, as well as the categorical features previously excluded from filtering, improve the performance of the model trained on the relevant features (see Figure 4.6).


| relevance              | features                                                                 |
|------------------------|---------------------------------------------------------------------------|
| relevant features     | last_event_distance/ time_since_last_event/distance_from_net / empty_net / rebound       |
| irrelevant features   | angle_change / period_time_seconds / time_since_powerplay |
| redundant features   | y_coord / x_coord / last_event_x  last_event_y                                           |
| candidate features     | shot_angle / event_speed / cat features               |

- *Figure 4.6. - Feature selection after filtering.*


\
**Feature Selection: Forward Wrapper**\
Our base model uses five features. For each feature added from the candidate set, we check whether the model’s performance improves. Based on these results, we select a total of nine **final features** whose combinatinon seems to give the best result: *last_event_distance*, *time_since_last_event*, *distance_from_net*, *empty_net*, *rebound*, *period*, *shot_type*, *last_event_type* and *player_count_diff*.\
We therefore proceed to train model 3 using the nine "final" features.



**Model-3**\
For model 3, we started from the previous best parameters (from model 2B) and refined the grid. We ran two rounds of cross-validation, adjusting the values, which resulted in the configuration (max_depth=6, eta=0.05, min_child_weight=5, subsample=0.8, colsample_bytree=1.0) with an AUC of 0.86 after 173 rounds, which we kept for training (figure 4.7). See the relevant WB log entry [here](https://wandb.ai/IFT6758_team4/milestone_2/runs/c79urnan?nw=nwuserantonioslagarias).


<table>
  <tr>
    <td>{% include milestone2/roc_model3.html width="400" %}</td>
    <td>{% include milestone2/percentile_model3.html width="400" %}</td>
  </tr>
  <tr>
    <td>{% include milestone2/cumulative_model3.html width="400" %}</td>
    <td>{% include milestone2/calibration_model3.html width="400" %}</td>
  </tr>
</table>


- *Figure 4.7. - The four figures for model-3. The model's AUC on the validation set is ~0.859*

Model-3’s overall performance is comparable to Model-2B, yet slightly worse overall. However, it uses fewer features, which could be useful for explainability. Thus, we test both Model 2B and Model 3 in step 6 (see below).

---

# **Step 5: Give it your best shot!**

In Q5, our goal was to build the strongest **expected-goals** model we could on the engineered dataset, prioritizing a fair comparison across methods rather than over-tuning one approach. We evaluated several learners (Logistic/Ridge, RandomForest, HistGradientBoosting, CatBoost), plus stacking/blending, n-seed ensembling, and simple mixture-of-experts splits, all on a fixed **80/20 stratified split (seed 42)**, similar to Q4. To keep results comparable, we locked a **17-feature** set derived from distance/angle and last-event context and reported metrics on the **validation** set only. The final choice is **CatBoost** with **3-fold OOF Platt calibration**: it provides the best single-model ranking and, crucially, more reliable probabilities for xG. The four figures below (ROC, goal-rate vs percentile, cumulative goals vs percentile, and reliability) summarize the validation performance; links to the tracked experiments are included alongside the plots.

## Setup

* **data:** `advanced_train.csv` (team FE upstream; we add a handful of derived fields and remove redudant / irrelevant feautures that reduce the model's performance).
* **target:** `is_goal` (1/0).
* **split:** stratified **80/20**, seed **42**. Calibration trains only on the train dataset; all plots/metrics are on the validation dataset.

## Features (locked 17)

* **base (7):** `distance_from_net, rebound, period, last_event_distance, shot_angle, shot_type, period_time_seconds`
* **context (4):** `time_since_last_event, angle_change, event_speed, last_event_type`
* **derived (6):** `log_distance, abs_angle, cos_angle, dist_x_abs_angle, rush(Δt≤2s), big_turn(|Δangle|≥30°)`
* **categorical:** `shot_type, last_event_type, period`

***Note:*** *(We intentionally excluded `time_since_powerplay`: it added only ~+0.002 AUC on test and we prioritized a stable, reproducible feature set.)*

## Final model choice

**CatBoost** (depth=8, lr=0.05, l2=3, 800 iters, early-stop 50, class-balanced) + **3-fold OOF Platt calibration**.
Calibration is trained on train-fold OOF scores and applied to validation probabilities (no leakage).

**validation (seed-42):**

* **catboost (calibrated):** **ROC-AUC ≈ 0.858**, **PR-AUC ≈ 0.525**
* **hgb comparator:** ROC-AUC ≈ 0.856, PR-AUC ≈ 0.525
* **random:** ROC-AUC ≈ 0.498

**why this model?**\
it edges out alternatives on ranking and—after Platt—delivers cleaner probability calibration, which matters directly for xG use.

---

## Question 5.1:

## Methods & Ablations

| Approach                        | Key settings                                  |   Val ROC-AUC |    Val PR-AUC | Outcome / Notes                                                                              |
| ------------------------------- | --------------------------------------------- | ------------: | ------------: | -------------------------------------------------------------------------------------------- |
| **CatBoost (calibrated)**       | depth=8, lr=0.05, l2=3, 800; OOF Platt    |    **0.8579** |    **0.5251** | **Final model**; AUC unchanged vs raw, **better reliability** (Brier≈0.0629, LogLoss≈0.2222) |
| CatBoost (raw)                  | same, no calibration                          |        0.8579 |        0.5251 | Matches calibrated AUC; kept to show calibration helps probability quality                   |
| **HistGradientBoosting (+OHE)** | max_leaf_nodes=63, lr=0.08, min_leaf=50       |        0.8561 |        0.5254 | Strong comparator, essentially tied on PR-AUC                                                |
| RF n-seed ensemble (5)      | 300 trees, balanced; seeds 42/101/202/303/404 |        0.8428 |        0.5042 | Slower, clearly below CatBoost/HGB                                                           |
| Ridge / Logistic                | regularized linear                            | ~0.71 (prior) | ~0.20 (prior) | Not re-run post-fix; underfits on this task                                                  |
| Blends / Stacking               | Cat+HGB(+RF); meta-LR                         |    ~0.85 |    ≈ CatBoost | No material gain; omitted from final curves                                                  |
| Mixture-of-Experts              | rebound / distance×rush splits                |    ~0.83 |    < CatBoost | Data fragmentation + overlap → worse                                                         |

**Interpretation.** Separability is **AUC ≈ 0.86 / PR-AUC ≈ 0.525** for the top two models (CatBoost and HGB). **Calibration** leaves AUC unchanged—as expected—but improves probability quality (lower Brier/LogLoss). Random Forest still lags, and added complexity (stacking/blends/MoE) doesn’t beat a single, calibrated CatBoost, so we chose the **calibrated CatBoost** as the final model.


## Required figures

{% include milestone2/q6_roc_val.html width="600" %}

* **roc:** both models are strong and nearly overlapping; **catboost (calibrated)** is a hair above **hgb** across most fpr. The very steep rise near the origin shows lots of easy positives; random is almost identical to the 45° line.

{% include milestone2/q6_goalrate_val.html width="600" %}

* **goal-rate vs percentile:** a clean monotonic drop. The top few percentiles have **very high goal rates** (spike at the far right), then decay smoothly. Catboost tracks or slightly beats hgb over most high-percentile bins; random is almost flat .

{% include milestone2/q6_cum_goals_val.html width="600" %}

* **cumulative goals:** the highest-scored shots account for a **large majority of goals**; catboost and hgb curves almost overlap with catboost just ahead most of the way. Random is much closer to a straight line.

{% include milestone2/q6_calibration_val.html width="600" %}

* **reliability:** both models are **reasonably calibrated** in the mid-range. Catboost is a bit **under-confident** at low–mid probabilities and then catches up; hgb hugs the diagonal longer but becomes **over-confident** in the highest bin (points above the line). Either way, calibrated catboost gives the tidiest probs overall.

---

## Question 5.2:
### W&B experiment links

- **Final model (CatBoost, calibrated) — run:** *[q5_catboost_final_calibrated](https://wandb.ai/IFT6758_team4/milestone_2/runs/kggfjh41?nw=nwuseraftabgazali003)*
- **Final model — artifact:** *[q5_catboost_final_calibrated:v2](https://wandb.ai/IFT6758_team4/milestone_2/artifacts/model/q6_catboost_final_calibrated/v2)*
- **HGB comparator — run:** *[q5_final_hgb_locked17](https://wandb.ai/IFT6758_team4/milestone_2/runs/rtzdqzn9?nw=nwuseraftabgazali003)*
- **CatBoost (raw) — run:** *[q5_catboost_raw_locked17](https://wandb.ai/IFT6758_team4/milestone_2/runs/jbxjv0j7?nw=nwuseraftabgazali003)*
- **nseed Random Forest - run:** *[q5_nseed_RF_locked17](https://wandb.ai/IFT6758_team4/milestone_2/runs/jamz7upp?nw=nwuseraftabgazali003)*

---

# **Step 6: Evaluate on test set!**

**Protocol.** We froze the models and hyper-parameters from earlier parts and evaluated on the untouched **2020/21 regular-season & playoff-seasons test split**. For each figure we plotted **six curves**: the three LR baselines from Q3, our **Q5 final CatBoost (Platt-calibrated)**, the **best performing XGboost** model (Model 3) and a random baseline for reference. We have merged baseline curves along with the best Q4 and Q5 models and we have also provided separate curves for the Q2 baseline models.

## Q6.1 — Regular season test results

**Headline scores (test):**

| Model                     |    ROC-AUC |     PR-AUC |      Brier |    LogLoss |
| ------------------------- | ---------: | ---------: | ---------: | ---------: |
| **XGboost-3** | **0.8749** | **0.5896** | **0.0567** | **0.2007** |
| **CatBoost (calibrated)** | **0.8719** | **0.5634** | **0.0620** | **0.2176** |
| LR-both (distance+angle)  |     0.7113 |     0.1898 |     0.0837 |     0.2994 |
| LR-distance               |     0.6955 |     0.1798 |     0.0845 |     0.3034 |
| LR-angle                  |     0.5568 |     0.1216 |     0.0878 |     0.3182 |


### Required figures (regular season)

{% include milestone2/tq7_general_test_roc_core4.html width="1000" %}

* **ROC/AUC.** XGBoost and CatBoost perform very well on the test dataset, with XGBoost resulting in a slightly higher **AUC≈0.875**. LR-both is the strongest of the three linear baselines but far behind. This mirrors validation ordering and actually improves on the split we used during development (good generalization).

{% include milestone2/tq7_general_test_goalrate_core4.html width="1000" %}

* **Goal rate vs percentile.** XGBoost and CatBoost’s curve are steep and clearly separated—the **top-scored shots exceed ~0.9 goals/shot** in the rightmost bins—while LR models decay slowly and compress probabilities. This indicates that the two best models concentrate true goals in the highest-confidence region much better compared to the baseline models.

{% include milestone2/tq7_general_test_cum_goals_core4.html %}

* **Cumulative goals.** XGBoost CatBoost capture a **much larger fraction of all goals** in the highest percentiles (curve well above the others). LR-both and LR-distance trail with noticeably lower lift; random is the expected diagonal. Again, XGBoost performs a bit better.

{% include milestone2/tq7_general_test_calibration_core4.html %}

* **Reliability.** CatBoost after Platt calibration tracks the diagonal through the mid-probability bins and reaches the upper-right corner (bins close to 1.0 map to near-100% observed). LR models under-predict (points sit below the diagonal) and remain poorly calibrated. XGBoost **underestimates** the probabilities of for shots with higher predictions.

**Below** are the required baseline models evaluated curves we have provided their interpretation above.

<table>
  <tr>
    <td>{% include milestone2/q7_general_test_roc_baselines.html width="520" %}</td>
    <td>{% include milestone2/q7_general_test_goalrate_baselines.html width="520" %}</td>
  </tr>
  <tr>
    <td>{% include milestone2/q7_general_test_cum_goals_baselines.html width="520" %}</td>
    <td>{% include milestone2/q7_general_test_calibration_baselines.html width="520" %}</td>
  </tr>
</table>

* **ROC:** `lr-both` best (≈0.71), `lr-distance` next (≈0.70), `lr-angle` weak (≈0.56).
* **Goal-rate:** clean monotonic drop; top bins: `lr-both` ≥ `lr-distance` » `lr-angle`.
* **Cumulative goals:** `lr-both` captures the most goals in high percentiles; `lr-angle` trails; random is near-diagonal.
* **Calibration:** all three are under-calibrated and probability-compressed (<0.3); `lr-both` is closest to the diagonal, `lr-angle` worst.
* **Takeaway:** combining distance+angle helps, but baselines remain far behind our Q4 and Q5 models.

---

## Q6.2 — Playoff games

**Headline scores (test):**

| Model                     |    ROC-AUC |     PR-AUC |      Brier |    LogLoss |
| ------------------------- | ---------: | ---------: | ---------: | ---------: |
| **XGboost-3** | **0.8551** | **0.5223** | **0.0540** | **0.1964** |
| **CatBoost (calibrated**) | **0.8514** | **0.4922** | **0.0598** | **0.2144** |
| LR-both (distance+angle)  |     0.6758 |     0.1478 |     0.0837 |     0.2826 |
| LR-distance               |     0.6758 |     0.1478 |     0.0766 |     0.2826 |
| LR-angle                  |     0.5674 |     0.1104 |     0.0786 |     0.2923 |


### Required figures (Playoffs season)

{% include milestone2/tq7_playoff_test_roc_core4.html width="700" %}

* **ROC/AUC.** **XGboost** remains the best performing model (**AUC ≈ 0.855**) followed by CatBoost (~0.851). LR-both (~0.696) > LR-distance (~0.676) >> LR-angle (~0.567). Pattern mirrors the regular season with a small drop for all models—ranking is preserved (good generalization).

{% include milestone2/tq7_playoff_test_goalrate_core4.html width="700" %}

* **Goal rate vs percentile.** XGBoost and CatBoost concentrate goals in the top bins—**~1.0 goals/shot** at the extreme right—while LR curves are much flatter (≤~0.22). Separation between the best two models and the LR baseline models widens in the high-confidence region.

{% include milestone2/tq7_playoff_test_cum_goals_core4.html width="700" %}

* **Cumulative goals.** XGBoost and CatBoost capture a **much larger share of goals** in the highest percentiles (curve far above LR lines). LR-both and LR-distance trail with moderate lift; LR-angle is the weakest. XGBoost performance is slightly better.

{% include milestone2/tq7_playoff_test_calibration_core4.html width="1000" %}

* **Reliability.** CatBoost (after Platt calibration) tracks the diagonal through mid-probabilities but **under-predicts at the extreme right** (points above the line → observed > predicted). LR baselines cluster at low probabilities and remain under-confident overall, with limited spread. XGBoost **overstimates** the chances of mid-range shots and **underestimates** the chances of shots with higher predictions.

**Below** are the required baseline models curves evaluated on playoff test, we have provided their interpretation above.

<table>
  <tr>
    <td>{% include milestone2/q7_playoff_test_roc_baselines.html width="520" %}</td>
    <td>{% include milestone2/q7_playoff_test_goalrate_baselines.html width="520" %}</td>
  </tr>
  <tr>
    <td>{% include milestone2/q7_playoff_test_cum_goals_baselines.html width="520" %}</td>
    <td>{% include milestone2/q7_playoff_test_calibration_baselines.html width="520" %}</td>
  </tr>
</table>

---

### **Conclusion — test evaluation**

**Regular Season**\
On the 2020/21 test set, **XGBoost-3** edges **CatBoost (calibrated)** by a **very small AUC margin** (**0.8749 vs 0.8719**, Δ≈+0.003) and shows higher **PR-AUC** (≈0.590 vs 0.563). Both boosted models are well ahead of the LR baselines. The lift vs. validation likely reflects the **season-based split** (train/test separated by season): the 2020/21 regular season appears slightly easier to predict (fewer outliers/lower variability). Compared with the validation split (AUC≈0.859), both boosted models **improve on test** (XGBoost **0.8749**, CatBoost **0.8719**). The LR baselines land **near or slightly below** their validation scores (e.g., LR-both ~0.71), so the boosts were **better than expected** for the tree models. A plausible reason is the **seasonwise split**: 2020/21 regular-season shots appear a bit easier (fewer outliers / lower variance). Despite minor differences in feature engineering and calibration, the models capture **very similar structure** and **separate** shot quality well—CatBoost provides well-calibrated probabilities, while XGBoost achieves a marginally higher ranking.

**Playoffs**\
On the 2020/21 **playoff** set, **XGBoost-3** reaches **AUC ≈ 0.855** (vs **0.8749** regular) and **CatBoost (calibrated)** **AUC ≈ 0.851** (vs **0.8719**). The LR baselines follow at **LR-both ≈ 0.696**, **LR-distance ≈ 0.676**, **LR-angle ≈ 0.567**. So all models dip modestly (~**0.02–0.03 AUC** for the boosted models), and the **ordering is unchanged** (**XGBoost ≳ CatBoost ≫ LR**). This drop is consistent with higher variance/tighter game states in playoffs; ROC/goal-rate/cumulative-goals/reliability show similar shapes to the regular season, indicating **good generalization** without any test-time tuning.

---
