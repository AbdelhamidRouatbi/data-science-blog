---
layout: post
title: Milestone 2
---
## Introduction: How to Predict a Goal?

Following the NHL data exploration of *Milestone 1*, we now dive deeper into the task of estimating the **quality of shots**. For this, we calculate the likelihood of  shot resulting in a goal (**Expected Goals**) based on available features, e.g. *shot type*, *distance*, *angle*, etc. This exercise consists of six steps:

- Step 1 introduces a first round of **feature engineering (I)** to prepare the dataset for linear regression models.
- Step 2 runs initial tests with **linear regression**.
- Step 3 presents a second round of **feature engineering (II)** to create advanced features for future models.
- Step 4 tries different **hyperparameter tuning** and **feature selection** methods to determine the best parameters and features, and trains three **XGBoost** models (see Figures 4.1…).
- Step 5 proposes the **best model** based on previous experiments.
- Step 6 presents the **test results**.

All experiments and the most relevant models are logged on our [Weights & Biases page](https://wandb.ai/IFT6758_team4/milestone_2) (WB).



---


# **Step 1: Feature Engineering I**


<figure>
  {% include milestone2/distance_histogram.html %}
  <figcaption>Figure 1.1: Shot distance distribution histogram.</figcaption>
</figure>
Figure 1.1 is a histogram showing how shot frequency varies with distance from the net. Goals are represented in red and missed shots in blue.
From this figure, we understand that most shots are taken close to the net. Frequency drops sharply as distance goes beyond 60 feet. The red region to the left indicates that goals occur mostly at short ranges, which means that proximity to the net is strongly correlated with scoring likelihood.

<figure>
  {% include milestone2/angle_histogram.html %}
  <figcaption>Figure 1.2: Shot angle distribution histogram.</figcaption>
</figure>
Figure 1.2 is a histogram showing how shot frequency varies by shot angle.
Most shots occur at narrow angles below 40°, where players easily face the net. As the angle widens, both total shots and goals decline sharply, indicating that shots from sharper angles are less frequent and less likely to result in goals.

<figure>
  {% include milestone2/distance_angle_histogram.html %}
  <figcaption>Figure 1.3: Shot distance-angle histogram.</figcaption>
</figure>
Figure 1.3 is a 2D histogram that shows the joint distribution of shot distance and shot angle.
Most shots cluster at short distances (under 50 feet) and moderate angles (below 40°). This means that players typically shoot frmo close range and relatively central positions. The density quickly decreases for longer distances and wider angles, indicating that shots far from the net or from extreme angles are rare.


<figure>
  {% include milestone2/goalrate_distance.html %}
  <figcaption>Figure 1.4: Goal rate by shot distance.</figcaption>
</figure>
Figure 1.4 shows how the goal rate as a function of shot distance.
The goal rate is highest very close to the net, then drops charply as distance increases, reaching a low and almost stable level beyond 50 units. This confirms that shots taken near the net have a much higher change of scoring, and that long-range attempts rarely result in goals. The small rise on the right of the plot likely comes from empty-net long-distance goals.


<figure>
  {% include milestone2/goalrate_angle.html %}
  <figcaption>Figure 1.5: Goal rate by shot angle.</figcaption>
</figure>
discuss
Figure 1.5 shows the goal rate as a function of shot angle.
Goalrate decreases steadily as the angle widens. Shots taken from more central positions are far more likely to score, and those from the sides of the net have a significantly lower goal probability. This shows that head-on shots are the most effective.

<figure>
  {% include milestone2/goals_distance.html %}
  <figcaption>Figure 1.6: Goals distance histogram.</figcaption>
</figure>
Figure 1.6 is a histogram of the distribution of goal distances from the net and distinguishes between goals on empty net and non-empty net.
We see that almost all non-empty net goals occur within 30-40 feet of the net. If we zoom on the right side of the figure (distance > 100), we notice that the empty-net long-distance goals dominate the non-empty net long-distance goals. This confirms the domain knowledge that it's extremely rare to score a non-empty net goal from one's own defensive zone, since such long shots are almost always taken when the opposing goalie leaves the net to join the attack.

We investigated the event with the highest distance non-empty net goal. The shot was taken at a distance of 187 feet, by Derek MacKenzie. After looking at a <a href="https://youtu.be/XaBII6f0f0o?si=PL3pMWz8S0F2ftsn&t=75">video</a> of the goal, we see that the shooting player is right next to the net. It is very clear that the distance is wrong. This means one of the features used to calculate distance from net is wrong.

We also investigate the highest distance goal on an empty net. From the <a href="https://youtu.be/c0zKHYiz18g?si=i6W4q13sDds_rbmQ&t=390">video</a> we confirm that it is indeed a long-distance goal on an empty net. We also verified that the shot type feature is correct.

Lastly, we check a goal that has an angle of 90°. For this, we picked a goal by Brian Dumoulin against Columbus Blue Jackets at the second period. The <a href="https://youtu.be/LFeb46Dba3A?si=A5DTq2JwJOC4wjd_&t=270">video</a> shows a very impressive shot from behind the net that bounces off a player to reach the back of the net and score a goal. We also confirmed other features with <a href="https://www.nhl.com/gamecenter/cbj-vs-pit/2017/04/04/2016021181">NHL gamecenter</a>.

---

# **Step 2: Baseline Models**

we trained three baseline **logistic regressions** on **distance only**, **angle only**, and **distance+angle**, plus a **random-uniform** baseline. evaluation used **validation probabilities** only.

**setup.** features: **distance_from_net**, **shot_angle**; target: **is_goal** (1 = goal). from `baseline_train.csv` we made a **stratified 80/20 split (seed 42)** and trained three default logistic regressions: (i) distance, (ii) angle, (iii) distance+angle. we also included a random-uniform baseline. missing values in the two features were imputed with the **median fitted on the training split**. the **test set was not used**.

**evaluation.** because we care about **probabilities** (expected goals), all curves use **validation-set predicted probabilities for class 1 (goal)**:

1. **roc curve (+ auc)** with a **chance (45°)** line
2. **goal rate vs model percentile**
3. **cumulative % of goals vs percentile**
4. **reliability (calibration) diagram**

**Findings:**

* **distance+angle** gives the best ranking (**AUC ≈ 0.707**), **distance** is close behind (**AUC ≈ 0.692**), **angle** is weak (**AUC ≈ 0.560**), and **random-uniform** sits at chance (**AUC ≈ 0.496**).
* high-probability percentiles have much higher **goal rates**; **distance+angle** stays above the others across most of the curve, with **distance** next and **angle** trailing; **random** is nearly flat.
* in the cumulative plot, **distance+angle** captures goals fastest, then **distance**, then **angle**; **random** increases almost linearly.
* calibration isn’t perfect: most predictions lie in a low-probability band, and points deviate from the diagonal (some under/over-confidence), but the ranking remains useful.

---

## **Question 2.1**

we split the data into training and validation using a stratified split so the goal rate is preserved. we then trained a default **logistic regression** on a single feature, **distance_from_net**, and evaluated **accuracy** on the validation set. the accuracy looks deceptively decent, but inspection of the predictions shows most probabilities are very small; with a 0.5 threshold, the model predicts almost all shots as “no goal.” because goals are relatively rare, a model can achieve high accuracy without actually identifying goals. **takeaway:** accuracy is a poor metric here; we should evaluate the **predicted probabilities** directly rather than hard 0/1 decisions.

## **Question 2.2**

motivated by q2.1, we use `predict_proba` and evaluate **validation probabilities** for the positive class (goal). we generated four diagnostics: roc/auc, goal-rate vs percentile, cumulative % of goals vs percentile, and a reliability diagram. these show that probability ranking is informative even when raw accuracy is misleading.

## **Question 2.3**

we repeated the setup for **angle only** and **distance+angle**, and added a **random-uniform** baseline. we produced the **same four figures** on the validation set, each with **four curves**.

* **roc/auc:** **distance+angle (≈ 0.707)** > **distance (≈ 0.692)** >> **angle (≈ 0.560)** > **random (≈ 0.496)**.
* **goal rate vs percentile:** top percentiles show the highest goal rates; ordering is **distance+angle** > **distance** > **angle** > **random** (random is nearly flat).
* **cumulative % of goals:** **distance+angle** accumulates goals fastest across percentiles, confirming stronger ranking than single-feature models.
* **reliability:** models are not perfectly calibrated; most predictions live around low probabilities, with some bins showing under/over-confidence (especially at higher predicted values where sample sizes are smaller).

**takeaway:** **distance** carries most of the signal; **adding angle** improves ranking further; **random** behaves as expected.

---

## **Question 2.4**

{% include milestone2/q2_fig_roc_auc.html width="600" %}

**Figure — ROC (+ AUC, ranking quality).** *lr-both* (**AUC ≈ 0.707**) ranks shots best; *lr-distance* (**AUC ≈ 0.692**) is next; *lr-angle* (**AUC ≈ 0.560**) is only slightly above chance; **random-uniform** (**AUC ≈ 0.496**) tracks the **chance (45°)** line.

---

{% include milestone2/q2_fig_goalrate_vs_percentile.html width="600" %}

**Figure — Goal rate vs model percentile.** Goal rate peaks at the **top percentiles** and declines steadily. Ordering across most of the curve is **lr-both** > **lr-distance** > **lr-angle**; **random-uniform** is relatively flat.

---

{% include milestone2/q2_fig_cum_goals_vs_percentile.html width="600" %}

**Figure — Cumulative % of goals vs percentile.** **lr-both** rises fastest (captures goals earliest), **lr-distance** follows closely, **lr-angle** lags, and **random-uniform** increases near linearly.

---

{% include milestone2/q2_fig_reliability_diagram.html width="600" %}

**Figure — Reliability (calibration) diagram.** Most predictions sit in a **low-probability band**; deviations from the diagonal indicate **imperfect calibration** (some under/over-confidence), which is typical for simple baselines. Despite this, ranking remains useful, as confirmed by the ROC and percentile plots.

--

- **Artifacts & runs.** The three models are saved as W&B **artifacts**, and each experiment run is linked below:

### Baseline Runs
- [lr-distance (run)](https://wandb.ai/IFT6758_team4/milestone_2/runs/26ipd8pc?nw=nwuseraftabgazali003)
- [lr-angle (run)](https://wandb.ai/IFT6758_team4/milestone_2/runs/4qcqb2hw?nw=nwuseraftabgazali003)
- [lr-both (run)](https://wandb.ai/IFT6758_team4/milestone_2/runs/3g7slfrs?nw=nwuseraftabgazali003)

### Saved Baseline Models (Artifacts)
- [lr-distance:v1](https://wandb.ai/IFT6758_team4/milestone_2/artifacts/model/lr-distance/v1)
- [lr-angle:v1](https://wandb.ai/IFT6758_team4/milestone_2/artifacts/model/lr-angle/v1)
- [lr-both:v1](https://wandb.ai/IFT6758_team4/milestone_2/artifacts/model/lr-both/v1)

**Note.** The **test set remains untouched** in this section; everything above uses the training/validation split only.


---
# **Step 3: Feature Engineering II**
In this step, we used the raw data to add more features to our dataset.

- **period_time_seconds** : Time elapsed in the current period, measured in seconds
- **period** : Current period of the game (1, 2, 3, or 4)
- **x_coord** : X-coordinate of the event on the rink
- **y_coord** : Y-coordinate of the event on the rink
- **distance_from_net** : Euclidean distance from the event location to the center of the net
- **shot_angle** : Angle between the shot location and the center of the goal
- **shot_type** : Type of shot taken (e.g., wrist, slap, snap, backhand)
- **empty_net** : Whether the goalie's net was empty during the play
- **last_event_type** : Type of the immediately preceding event
- **last_event_x** : X-coordinate of the previous event
- **last_event_y** : Y-coordinate of the previous event
- **time_since_last_event** : Time (in seconds) elapsed since the last recorded event
- **last_event_distance** : Distance between the current event and the previous event
- **rebound** : Whether the shot was taken shortly after a previous shot (by the same team).
- **angle_change** : Change in shot angle compared to the previous shot
- **event_speed** : Speed of play between the last event and the current one
- **friendly_player_count** : Number of players on the shooter's team on the rink
- **opponent_player_count** : Number of players on the opposing team on the rink
- **player_count_diff** : Difference between friendly and opponent player counts (on the rink)
- **time_since_powerplay** : Time elapsed since the start of the most recent power play
- **is_goal** : Target variable indicating whether the shot resulted in a goal

We created the features from all the questions (including bonus power-play features). We also added another feature, **player_count_diff**, which is the difference between **friendly_player_count** and **opponent_player_count**. The experiment for the Winnipeg vs Washington game dataframe can be viewed <a href="https://wandb.ai/IFT6758_team4/milestone_2/runs/rgzgn2vx?nw=nwuserabdelhamidrouatbi">here</a>.

---

# **Step 4: Advanced Models**
In this exercise, we test three approaches to training XGBoost classifiers for predicting the most dangerous shots. Our first baseline model (1) predicts the likelihood of a shot resulting in a goal based only on its *distance* and *shot angle*, without any hyperparameter fine-tuning. The second model (2A, using two input features) includes only numerical features, while the third model (2B) incorporates all 14 available features. For both, we apply hyperparameter fine-tuning with gread search and cross-validation. We then aim to identify the relevant, redundant, and weak features using information-theory methods. We compute information gain for each feature and mutual information for each pair to make a first selection, and then confirms the selection through both forward and backward wrappers. We end up selecting seven features as the most relevant and train our final model (3) on this set.

Since XGBoost can handle missing values natively, we ensure that all missing entries are formatted as np.nan and rely on the model’s internal mechanisms for handling them. For each model tested, we present the same four figures used in Question 3: ROC/AUC, Goal rate vs. probability percentile, Cumulative proportion of goals vs. probability percentile, and Reliability curve.


## Question 4.1: Baseline XGBoost classifier

**Model-1**


<table>
  <tr>
    <td>{% include milestone2/roc_model1.html width="400" %}</td>
    <td>{% include milestone2/percentile_model1.html width="400" %}</td>
  </tr>
  <tr>
    <td>{% include milestone2/cumulative_model1.html width="400" %}</td>
    <td>{% include milestone2/calibration_model1.html width="400" %}</td>
  </tr>
</table>


- *Figure 4.1. - The four figures for the baseline model (Model-1). The ROC curve shows an AUC of ~0.71 for the validation set. This indicates that the model can distinguish to some degree higher-quality shots from low-quality ones since it performs better than a random guess would (AUC=0.5).*

The percentile curve (top right) behaves as expected as the actual goal rate increases for shots in bins with higher predicted probabilities. For example, among the 10% of shots the model identified as most dangerous, 22.5% were actual goals. Similarly, the top 20% of shots by predicted xG account for 40% of all goals, and the top 40% account for 75% of all goals (bottom left). However, the calibration curve (bottom right) indicates that shots the model predicted as more likely to score actually scored less often than those with lower predicted probabilities.\
See the relevant WB log entry [here](https://wandb.ai/IFT6758_team4/milestone_2/runs/3j35f7mq?nw=nwuserantonioslagarias).

## Question 4.2: Full XGBoost classifier

XGBoost can handle both numerical and categorical features. To assess its performance on datasets containing a mix of feature types — such as our *advanced_data* — we test two models. First, we exclude the two categorical features, *shot type* and *previous event type* (model 2A, with 12 features). Then, we include them in model 2B, which has access to all 14 features. For both models, we optimize the hyperparameters using the same grid search approach with cross-validation (CV).

**Model-2A**

For model-2A, we ran cross-validation three times, adjusting the grid, optimizing for AUC. First, a 3-fold CV gave us the best configuration as (max_depth=5, eta=0.1, min_child_weight=5, subsample=1.0, colsample_bytree=0.8) with an AUC of 0.733 (73 rounds). A second 5-fold CV gave us (max_depth=5, eta=0.05, min_child_weight=1, subsample=1.0, colsample_bytree=0.8) with an AUC of 0.7334 (99 rounds). The third 5-fold CV produced the same configuration (max_depth=5, eta=0.05, min_child_weight=1, subsample=1.0, colsample_bytree=0.8) with an AUC of 0.7336 (122 rounds), which we kept for training (see figure 4.2).\
See the relevant WB log entry [here](https://wandb.ai/IFT6758_team4/milestone_2/runs/escltr3d/artifacts?nw=nwuserantonioslagarias).


<table>
  <tr>
    <td>{% include milestone2/roc_model2A.html width="400" %}</td>
    <td>{% include milestone2/percentile_model2A.html width="400" %}</td>
  </tr>
  <tr>
    <td>{% include milestone2/cumulative_model2A.html width="400" %}</td>
    <td>{% include milestone2/calibration_model2A.html width="400" %}</td>
  </tr>
</table>

- *Figure 4.2. - The four figures for model-2A. The model's AUC on the validation set is ~0.735.*

This is a better performance compared to the baseline model. Among the 10% of shots the model considered most dangerous, 25% were actual goals, an improvement compared to the baseline model (bottom left). Similarly, the top 20% of shots predicted by XGBoost account for 45% of all goals, while the top 40% account for 75% of all goals, a slight improvement over the baseline model. (top right). The calibration curve is also improved.
0.237



**Model 2B**
For model2B we started from the previous best parameters for (2A), and adjusted the grid to see if there is any change. We run a single cross-validation that gave us the best configuration as (max_depth=5, eta=0.75, min_child_weight=5, subsample=1.0, colsample_bytree=0.9) with an AUC of 0.739 (143 rounds) which we kept for training (Figure 4.3).\
See the relevant WB log entry [here](https://wandb.ai/IFT6758_team4/milestone_2/runs/9whkzrdq?nw=nwuserantonioslagarias).



<table>
  <tr>
    <td>{% include milestone2/roc_model2B.html width="400" %}</td>
    <td>{% include milestone2/percentile_model2B.html width="400" %}</td>
  </tr>
  <tr>
    <td>{% include milestone2/cumulative_model2B.html width="400" %}</td>
    <td>{% include milestone2/calibration_model2B.html width="400" %}</td>
  </tr>
</table>


- *Figure 4.3. - The four figures for model-2B. The model's AUC on the validation set is ~0.743*

Model 2B’s performance differs slightly from that of Model 2A. Its overall AUC is higher, however, among the 10% of shots the model identified as most dangerous, 23.7% were actual goals (compared to 25% for Model 2A). Similarly, the cumulative curve shows a slight improvement, while the calibration curve performs slightly worse. Therefore, we decided to register both models for testing in Step 6.




## Question 4.3: Optimised XGBoost classifier

**Handling Missing Values**\
To train an optimised Model-3, we need to examine the features to identify those most relevant for predicting goals. We begin with two filtering methods based on information theory, **information** gain and **mutual information**. For this step, the two categorical features (*shot type* and *last event type*) are excluded. Missing values in continuous numerical features are filled with their mean value, while missing values in the single boolean feature *rebound* are replaced with the most frequent value.

To also assess the relevance of categorical features, after selecting an initial feature set through information-theory filtering, we perform both **forward** and **backward wrapper** searches. These tests evaluate how individual features affect the performance of a base model, allowing us to determine the optimal set of features.


**Feature Selection: Filtering**\
We first uses information theory to better understand how each feature contributes to information gain. Figure 4.4 presents the four most significant features (left) and the four least significant ones (right).

| features (best)         | i_score  |               | features (worst)         | i_score  |
|--------------------------|-----------|-------------|--------------------------|-----------|
| distance_from_net        | 0.024107  |             | last_event_x             | 0.003538  |
| rebound                  | 0.023004  |             | time_since_last_event    | 0.002954  |
| period                   | 0.021879  |             | period_time_seconds      | 0.002904  |
| y_coord                  | 0.019276  |             | angle_change             | 0.000104  |

- *Figure 4.4. - The best and worst features in terms of information gain.*

\
However, we observe that some significant features appear redundant, such as *distance* and *y_coord*. Therefore, we proceed by testing all possible pairs of features to measure their mutual information. Figure 4.5 shows the pairs with the highest mutual information values.

| combination                                | mi       |   | combination                                | mi       |
|--------------------------------------------|-----------|---|--------------------------------------------|-----------|
| distance_from_net & shot_angle             | 7.019193  |   | y_coord & shot_angle                       | 2.818636  |
| last_event_distance & event_speed          | 4.472923  |   | time_since_last_event & event_speed        | 1.718200  |
| x_coord & distance_from_net                | 4.427126  |   | rebound & angle_change                     | 0.654856  |


- *Figure 4.5. - The pairs sharing the most mutual information.*

\
Based on these observations, we can separate the most relevant features from the redundant and irrelevant ones. However, the distinction is not always clear, so we proceed with a forward wrapper method to test whether the candidate features, as well as the categorical features previously excluded from filtering, improve the performance of the model trained on the relevant features (see Figure 4.6).


| relevance              | features                                                                 |
|------------------------|---------------------------------------------------------------------------|
| relevant features     | distance_from_net / rebound / period / last_event_distance              |
| irrelevant features   | angle_change / time_since_last_event / period_time_seconds / last_event_x |
| redundant features*    | y_coord / x_coord                                                       |
| candidate features     | shot_angle / event_speed / shot_type / last_event_type                  |

- *Figure 4.6. - The best and worst features in terms of information gain.*


\
**Feature Selection: Forward and Backward Wrapper**\
Our base model uses four features. For each feature added from the candidate set, we check whether the model’s performance improves. Based on these results, we select a total seven "final" features whose combinatinon gives the best results: distance_from_net, rebound, period, last_event_distance, shot_angle, shot_type, and period_time_seconds.

Finally, we run a single backward wrapper starting with all 14 features used in model 2B, removing one feature each time. We observe that removing any of the “final” features significantly worsens the model’s performance, while removing any other feature has only a minimal impact. We therefore proceed to train model 3 using the seven "final" features.



**Model-3**

For model 3, we started from the previous best parameters (from model 2B) and refined the grid. We ran two rounds of cross-validation, adjusting the values, which resulted in the configuration (max_depth=6, eta=0.06, min_child_weight=5, subsample=1, colsample_bytree=0.8) with an AUC of 0.741 after 155 rounds, which we kept for training (figure 4.7). See the relevant WB log entry [here](https://wandb.ai/IFT6758_team4/milestone_2/runs/6rf5t2xm?nw=nwuserantonioslagarias).


<table>
  <tr>
    <td>{% include milestone2/roc_model3.html width="400" %}</td>
    <td>{% include milestone2/percentile_model3.html width="400" %}</td>
  </tr>
  <tr>
    <td>{% include milestone2/cumulative_model3.html width="400" %}</td>
    <td>{% include milestone2/calibration_model3.html width="400" %}</td>
  </tr>
</table>


- *Figure 4.7. - The four figures for model-3. The model's AUC on the validation set is ~0.743*

Model-3’s overall performance is comparable to both Model-2A and Model-2B. Despite using fewer features, it maintains the best AUC achieved by Model 2B, and its cumulative and percentile curves closely resemble those of the best Model-2A. Although the calibration curve is not ideal, its fluctuations are similar to those observed in Model 2B.

---

# **Q6 — Give it your best shot!**

In Q6, our goal was to build the strongest **expected-goals** model we could on the engineered dataset, prioritizing a fair comparison across methods rather than over-tuning one approach. We evaluated several learners (Logistic/Ridge, RandomForest, HistGradientBoosting, CatBoost), plus stacking/blending, n-seed ensembling, and simple mixture-of-experts splits, all on a fixed **80/20 stratified split (seed 42)**. To keep results comparable, we locked a **17-feature** set derived from distance/angle and last-event context and reported metrics on the **validation** set only. The final choice is **CatBoost** with **3-fold OOF Platt calibration**: it provides the best single-model ranking and, crucially, more reliable probabilities for xG. The four figures below (ROC, goal-rate vs percentile, cumulative goals vs percentile, and reliability) summarize the validation performance; links to the tracked experiments are included alongside the plots.

## Setup

* **data:** `advanced_train.csv` (team FE upstream; we add a handful of derived fields).
* **target:** `is_goal` (1/0).
* **split:** stratified **80/20**, seed **42**. Calibration trains on **train-only** OOF; all plots/metrics are on **validation**.

## Features (locked 17)

* **base (7):** `distance_from_net, rebound, period, last_event_distance, shot_angle, shot_type, period_time_seconds`
* **context (4):** `time_since_last_event, angle_change, event_speed, last_event_type`
* **derived (6):** `log_distance, abs_angle, cos_angle, dist_x_abs_angle, rush(Δt≤2s), big_turn(|Δangle|≥30°)`
* **categorical:** `shot_type, last_event_type, period`

## Final model choice

**CatBoost** (depth=8, lr=0.05, l2=3, 800 iters, early-stop 50, class-balanced) + **3-fold OOF Platt calibration**.
Calibration is trained on train-fold OOF scores and applied to validation probabilities (no leakage).

**validation (seed-42):**

* **catboost (calibrated):** **ROC-AUC ≈ 0.858**, **PR-AUC ≈ 0.525**
* **hgb comparator:** ROC-AUC ≈ 0.856, PR-AUC ≈ 0.525
* **random:** ROC-AUC ≈ 0.498

**why this model?** it edges out alternatives on ranking and—after Platt—delivers cleaner probability calibration, which matters directly for xG use.

---

## Question 6.1:

## Methods & Ablations

| Approach                        | Key settings                                  |   Val ROC-AUC |    Val PR-AUC | Outcome / Notes                                                                              |
| ------------------------------- | --------------------------------------------- | ------------: | ------------: | -------------------------------------------------------------------------------------------- |
| **CatBoost (calibrated)**       | depth=8, lr=0.05, l2=3, 800; **OOF Platt**    |    **0.8579** |    **0.5251** | **Final model**; AUC unchanged vs raw, **better reliability** (Brier≈0.0629, LogLoss≈0.2222) |
| CatBoost (raw)                  | same, no calibration                          |        0.8579 |        0.5251 | Matches calibrated AUC; kept to show calibration helps probability quality                   |
| **HistGradientBoosting (+OHE)** | max_leaf_nodes=63, lr=0.08, min_leaf=50       |        0.8561 |        0.5254 | Strong comparator, essentially tied on PR-AUC                                                |
| **RF n-seed ensemble (5)**      | 300 trees, balanced; seeds 42/101/202/303/404 |        0.8428 |        0.5042 | Slower, clearly below CatBoost/HGB                                                           |
| Ridge / Logistic                | regularized linear                            | ~0.71 (prior) | ~0.20 (prior) | Not re-run post-fix; underfits on this task                                                  |
| Blends / Stacking               | Cat+HGB(+RF); meta-LR                         |    ~0.85 |    ≈ CatBoost | No material gain; omitted from final curves                                                  |
| Mixture-of-Experts              | rebound / distance×rush splits                |    ~0.83 |    < CatBoost | Data fragmentation + overlap → worse                                                         |

**Interpretation.** After the dataset fix, separability jumps to **AUC ≈ 0.86 / PR-AUC ≈ 0.525** for the top two models (CatBoost, HGB). **Calibration** leaves AUC unchanged—as expected—but improves probability quality (lower Brier/LogLoss), which matters for xG usage. Random Forest still lags, and added complexity (stacking/blends/MoE) doesn’t beat a single, calibrated CatBoost, so we ship the **calibrated CatBoost** as the final model.


## Required figures

{% include milestone2/q6_roc_val.html width="600" %}

* **roc:** both models are strong and nearly overlapping; **catboost (calibrated)** is a hair above **hgb** across most fpr, matching the legend (~0.858 vs ~0.856). The very steep rise near the origin shows lots of easy positives; random is the 45° line.

{% include milestone2/q6_goalrate_val.html width="600" %}

* **goal-rate vs percentile:** a clean monotonic drop. The top few percentiles have **very high goal rates** (spike at the far right), then decay smoothly. Catboost tracks or slightly beats hgb over most high-percentile bins; random is flat ~constant.

{% include milestone2/q6_cum_goals_val.html width="600" %}

* **cumulative goals:** the highest-scored shots account for a **large majority of goals**; catboost and hgb curves almost overlap with catboost just ahead most of the way. Random is much closer to a straight line.

{% include milestone2/q6_calibration_val.html width="600" %}

* **reliability:** both models are **reasonably calibrated** in the mid-range. Catboost is a bit **under-confident** at low–mid probabilities and then catches up; hgb hugs the diagonal longer but becomes **over-confident** in the highest bin (points above the line). Either way, calibrated catboost gives the tidiest probs overall.

---

## Question 6.2:
### W&B experiment links

- **Final model (CatBoost, calibrated) — run:** *[q6_catboost_final_calibrated](https://wandb.ai/IFT6758_team4/milestone_2/runs/kggfjh41?nw=nwuseraftabgazali003)*
- **Final model — artifact:** *[q6_catboost_final_calibrated:v2](https://wandb.ai/IFT6758_team4/milestone_2/artifacts/model/q6_catboost_final_calibrated/v2)*
- **HGB comparator — run:** *[q6_final_hgb_locked17](https://wandb.ai/IFT6758_team4/milestone_2/runs/rtzdqzn9?nw=nwuseraftabgazali003)*
- **CatBoost (raw) — run:** *[q6_catboost_raw_locked17](https://wandb.ai/IFT6758_team4/milestone_2/runs/jbxjv0j7?nw=nwuseraftabgazali003)*
- **nseed Random Forest - run:** *[q6_nseed_RF_locked17](https://wandb.ai/IFT6758_team4/milestone_2/runs/jamz7upp?nw=nwuseraftabgazali003)*

---

# **Q7 — Evaluate on test set!**

**Protocol.** We froze the models and hyper-parameters from earlier parts and evaluated on the untouched **2020/21 regular-season & playoff-seasons test split**. For each figure we plotted **five curves**: the three LR baselines from Q3, our **Q6 final CatBoost (Platt-calibrated)**, and a random baseline for reference. **Note** for interpretability sake, we have merged baseline curves along with the best Q6 model, however, we have also provided separate baseline only curves below as requested!

## Q7.1 — Regular season test results

**Headline scores (test):**

| Model                     |    ROC-AUC |     PR-AUC |      Brier |    LogLoss |
| ------------------------- | ---------: | ---------: | ---------: | ---------: |
| **CatBoost (calibrated)** | **0.8719** | **0.5634** | **0.0620** | **0.2176** |
| LR-both (distance+angle)  |     0.7113 |     0.1898 |     0.0837 |     0.2994 |
| LR-distance               |     0.6955 |     0.1798 |     0.0845 |     0.3034 |
| LR-angle                  |     0.5568 |     0.1216 |     0.0878 |     0.3182 |

### Required figures (regular season)

{% include milestone2/q7_general_test_roc_core4.html width="700" %}

* **ROC/AUC.** CatBoost dominates the LR baselines across the full FPR range (**AUC ≈ 0.872**). LR-both is the strongest of the three linear baselines but far behind. This mirrors validation ordering and actually **improves** on the split we used during development (good generalization).

{% include milestone2/q7_general_test_goalrate_core4.html width="700" %}

* **Goal rate vs percentile.** CatBoost’s curve is steep and clearly separated—the **top-scored shots exceed ~0.9 goals/shot** in the rightmost bins—while LR models decay slowly and compress probabilities. This indicates CatBoost concentrates true goals in the highest-confidence region much better.

{% include milestone2/q7_general_test_cum_goals_core4.html width="700" %}

* **Cumulative goals.** CatBoost captures a **much larger fraction of all goals** in the highest percentiles (curve well above the others). LR-both and LR-distance trail with noticeably lower lift; random is the expected diagonal.

{% include milestone2/q7_general_test_calibration_core4.html width="700" %}

* **Reliability.** CatBoost after **Platt calibration** tracks the diagonal through the mid-probability bins and reaches the upper-right corner (bins close to 1.0 map to near-100% observed). LR models **under-predict** (points sit below the diagonal) and remain poorly calibrated.

**Below** are the required baseline models evaluated curves we have provided their interpretation above.

<table>
  <tr>
    <td>{% include milestone2/q7_general_test_roc_baselines.html width="520" %}</td>
    <td>{% include milestone2/q7_general_test_goalrate_baselines.html width="520" %}</td>
  </tr>
  <tr>
    <td>{% include milestone2/q7_general_test_cum_goals_baselines.html width="520" %}</td>
    <td>{% include milestone2/q7_general_test_calibration_baselines.html width="520" %}</td>
  </tr>
</table>

* **ROC:** `lr-both` best (≈0.71), `lr-distance` next (≈0.70), `lr-angle` weak (≈0.56).
* **Goal-rate:** clean monotonic drop; top bins: `lr-both` ≥ `lr-distance` » `lr-angle`.
* **Cumulative goals:** `lr-both` captures the most goals in high percentiles; `lr-angle` trails; random is near-diagonal.
* **Calibration:** all three are under-calibrated and probability-compressed (<0.3); `lr-both` is closest to the diagonal, `lr-angle` worst.
* **Takeaway:** combining distance+angle helps, but baselines remain far behind our Q6 model.

---

## Q7.2 — Playoff games

**Headline scores (test):**

| Model                     |    ROC-AUC |     PR-AUC |      Brier |    LogLoss |
| ------------------------- | ---------: | ---------: | ---------: | ---------: |
| **CatBoost (calibrated)** | **0.8514** | **0.4922** | **0.0598** | **0.2144** |
| LR-both (distance+angle)  |     0.6758 |     0.1478 |     0.0837 |     0.2826 |
| LR-distance               |     0.6758 |     0.1478 |     0.0766 |     0.2826 |
| LR-angle                  |     0.5674 |     0.1104 |     0.0786 |     0.2923 |

### Required figures (Playoffs season)

{% include milestone2/q7_playoff_test_roc_core4.html width="700" %}

* **ROC/AUC.** CatBoost stays clearly ahead (**AUC ≈ 0.851**). LR-both (~0.696) > LR-distance (~0.676) >> LR-angle (~0.567). Pattern mirrors the regular season with a small drop for all models—ranking is preserved (good generalization).

{% include milestone2/q7_playoff_test_goalrate_core4.html width="700" %}

* **Goal rate vs percentile.** CatBoost concentrates goals in the top bins—**~1.0 goals/shot** at the extreme right—while LR curves are much flatter (≤~0.22). Separation between CatBoost and LR baselines widens in the high-confidence region.

{% include milestone2/q7_playoff_test_cum_goals_core4.html width="700" %}

* **Cumulative goals.** CatBoost captures a **much larger share of goals** in the highest percentiles (curve far above LR lines). LR-both and LR-distance trail with moderate lift; LR-angle is the weakest. This matches the ROC ordering.

{% include milestone2/q7_playoff_test_calibration_core4.html width="700" %}

* **Reliability.** CatBoost (after **Platt calibration**) tracks the diagonal through mid-probabilities but **under-predicts at the extreme right** (points above the line → observed > predicted). LR baselines cluster at low probabilities and remain under-confident overall, with limited spread.

**Below** are the required baseline models curves evaluated on playoff test, we have provided their interpretation above.

<table>
  <tr>
    <td>{% include milestone2/q7_playoff_test_roc_baselines.html width="520" %}</td>
    <td>{% include milestone2/q7_playoff_test_goalrate_baselines.html width="520" %}</td>
  </tr>
  <tr>
    <td>{% include milestone2/q7_playoff_test_cum_goals_baselines.html width="520" %}</td>
    <td>{% include milestone2/q7_playoff_test_calibration_baselines.html width="520" %}</td>
  </tr>
</table>

---

## **Conclusion — test evaluation**

On the **regular-season** test set, the calibrated CatBoost is clearly best (**AUC≈0.872, PR≈0.56**), with LR-both > LR-distance >> LR-angle and large lift in the highest-percentile bins.
On the **playoffs**, every model drops a little (CatBoost **AUC≈0.851**), but the **ranking is identical** and CatBoost still concentrates the largest share of goals in its top bins.
Calibration is stable: CatBoost tracks the diagonal in mid-probabilities but **under-predicts at the extreme right** on both splits; the LR baselines remain compressed and under-confident.
Overall, the models **generalize well** from validation to both test settings; the small playoff dip likely reflects a shift in shot mix/game context rather than overfitting.
We did not tune on test, so these numbers represent our **final, honest** out-of-sample performance.

---


# Sources / references


