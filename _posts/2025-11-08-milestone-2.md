---
layout: post
title: Milestone 2
---
## Introduction: How to Predict a Goal? 

Following the NHL data exploration of *Milestone 1*, we now dive deeper into the task of estimating the **quality of shots**. For this, we calculate the likelihood of  shot resulting in a goal (**Expected Goals**) based on available features, e.g. *shot type*, *distance*, *angle*, etc. This exercise consists of six steps:

- Step 1 introduces a first round of **feature engineering (I)** to prepare the dataset for linear regression models.  
- Step 2 runs initial tests with **linear regression**.  
- Step 3 presents a second round of **feature engineering (II)** to create advanced features for future models.  
- Step 4 tries different **hyperparameter tuning** and **feature selection** methods to determine the best parameters and features, and trains three **XGBoost** models (see Figures 4.1…). 
- Step 5 proposes the **best model** based on previous experiments.  
- Step 6 presents the **test results**.

All experiments and the most relevant models are logged on our [Weights & Biases page](https://wandb.ai/IFT6758_team4/milestone_2) (WB).



---


# **Step 1: Feature Engineering I**

## Question 1.1 : 
## Question 1.2 :
## Question 1.3 : 

<!-- ![Figure 1.1](assets/images/().png) -->

- *Figure 1.1 - *

---
# **Step 2: Baseline Models**

## Question 2.1 : 
## Question 2.2 :
## Question 2.3 : 
## Question 2.4 :

<!-- ![Figure 2.1](assets/images/screenshot.png) -->

- *Figure 2.1 - *

---
# **Step 3: Feature Engineering II**

## Question 3.1 : 
## Question 3.2 :
## Question 3.3 : 
## Question 3.4 :
## Question 3.5 :

<!-- ![Figure 3.1](assets/images/screenshot.png) -->

- *Figure 3.1 - *


---

# **Step 4: Advanced Models**
In this exercise, we test three approaches to training XGBoost classifiers for predicting the most dangerous shots. Our first baseline model (1) predicts the likelihood of a shot resulting in a goal based only on its *distance* and *shot angle*, without any hyperparameter fine-tuning. The second model (2A, using two input features) includes only numerical features, while the third model (2B) incorporates all 14 available features. For both, we apply hyperparameter fine-tuning with gread search and cross-validation. We then aim to identify the relevant, redundant, and weak features using information-theory methods. We compute information gain for each feature and mutual information for each pair to make a first selection, and then confirms the selection through both forward and backward wrappers. We end up selecting seven features as the most relevant and train our final model (3) on this set.

Since XGBoost can handle missing values natively, we ensure that all missing entries are formatted as np.nan and rely on the model’s internal mechanisms for handling them. For each model tested, we present the same four figures used in Question 3: ROC/AUC, Goal rate vs. probability percentile, Cumulative proportion of goals vs. probability percentile, and Reliability curve.


## Question 4.1: Baseline XGBoost classifier

**Model-1**


<table>
  <tr>
    <td>{% include milestone2/roc_model1.html width="400" %}</td>
    <td>{% include milestone2/percentile_model1.html width="400" %}</td>
  </tr>
  <tr>
    <td>{% include milestone2/cumulative_model1.html width="400" %}</td>
    <td>{% include milestone2/calibration_model1.html width="400" %}</td>
  </tr>
</table>


- *Figure 4.1. - The four figures for the baseline model (Model-1). The ROC curve shows an AUC of ~0.71 for the validation set. This indicates that the model can distinguish to some degree higher-quality shots from low-quality ones since it performs better than a random guess would (AUC=0.5).*

The percentile curve (top right) behaves as expected as the actual goal rate increases for shots in bins with higher predicted probabilities. For example, among the 10% of shots the model identified as most dangerous, 22.5% were actual goals. Similarly, the top 20% of shots by predicted xG account for 40% of all goals, and the top 40% account for 75% of all goals (bottom left). However, the calibration curve (bottom right) indicates that shots the model predicted as more likely to score actually scored less often than those with lower predicted probabilities.\
See the relevant WB log entry [here](https://wandb.ai/IFT6758_team4/milestone_2/runs/3j35f7mq?nw=nwuserantonioslagarias).

## Question 4.2: Full XGBoost classifier

XGBoost can handle both numerical and categorical features. To assess its performance on datasets containing a mix of feature types — such as our *advanced_data* — we test two models. First, we exclude the two categorical features, *shot type* and *previous event type* (model 2A, with 12 features). Then, we include them in model 2B, which has access to all 14 features. For both models, we optimize the hyperparameters using the same grid search approach with cross-validation (CV).

**Model-2A**

For model-2A, we ran cross-validation three times, adjusting the grid, optimizing for AUC. First, a 3-fold CV gave us the best configuration as (max_depth=5, eta=0.1, min_child_weight=5, subsample=1.0, colsample_bytree=0.8) with an AUC of 0.733 (73 rounds). A second 5-fold CV gave us (max_depth=5, eta=0.05, min_child_weight=1, subsample=1.0, colsample_bytree=0.8) with an AUC of 0.7334 (99 rounds). The third 5-fold CV produced the same configuration (max_depth=5, eta=0.05, min_child_weight=1, subsample=1.0, colsample_bytree=0.8) with an AUC of 0.7336 (122 rounds), which we kept for training (see figure 4.2).\
See the relevant WB log entry [here](https://wandb.ai/IFT6758_team4/milestone_2/runs/escltr3d/artifacts?nw=nwuserantonioslagarias).


<table>
  <tr>
    <td>{% include milestone2/roc_model2A.html width="400" %}</td>
    <td>{% include milestone2/percentile_model2A.html width="400" %}</td>
  </tr>
  <tr>
    <td>{% include milestone2/cumulative_model2A.html width="400" %}</td>
    <td>{% include milestone2/calibration_model2A.html width="400" %}</td>
  </tr>
</table>

- *Figure 4.2. - The four figures for model-2A. The model's AUC on the validation set is ~0.735.*

This is a better performance compared to the baseline model. Among the 10% of shots the model considered most dangerous, 25% were actual goals, an improvement compared to the baseline model (bottom left). Similarly, the top 20% of shots predicted by XGBoost account for 45% of all goals, while the top 40% account for 75% of all goals, a slight improvement over the baseline model. (top right). The calibration curve is also improved.
0.237



**Model 2B**
For model2B we started from the previous best parameters for (2A), and adjusted the grid to see if there is any change. We run a single cross-validation that gave us the best configuration as (max_depth=5, eta=0.75, min_child_weight=5, subsample=1.0, colsample_bytree=0.9) with an AUC of 0.739 (143 rounds) which we kept for training (Figure 4.3).\
See the relevant WB log entry [here](https://wandb.ai/IFT6758_team4/milestone_2/runs/9whkzrdq?nw=nwuserantonioslagarias).



<table>
  <tr>
    <td>{% include milestone2/roc_model2B.html width="400" %}</td>
    <td>{% include milestone2/percentile_model2B.html width="400" %}</td>
  </tr>
  <tr>
    <td>{% include milestone2/cumulative_model2B.html width="400" %}</td>
    <td>{% include milestone2/calibration_model2B.html width="400" %}</td>
  </tr>
</table>


- *Figure 4.3. - The four figures for model-2B. The model's AUC on the validation set is ~0.743*

Model 2B’s performance differs slightly from that of Model 2A. Its overall AUC is higher, however, among the 10% of shots the model identified as most dangerous, 23.7% were actual goals (compared to 25% for Model 2A). Similarly, the cumulative curve shows a slight improvement, while the calibration curve performs slightly worse. Therefore, we decided to register both models for testing in Step 6.




## Question 4.3: Optimised XGBoost classifier

**Handling Missing Values**\
To train an optimised Model-3, we need to examine the features to identify those most relevant for predicting goals. We begin with two filtering methods based on information theory, **information** gain and **mutual information**. For this step, the two categorical features (*shot type* and *last event type*) are excluded. Missing values in continuous numerical features are filled with their mean value, while missing values in the single boolean feature *rebound* are replaced with the most frequent value.

To also assess the relevance of categorical features, after selecting an initial feature set through information-theory filtering, we perform both **forward** and **backward wrapper** searches. These tests evaluate how individual features affect the performance of a base model, allowing us to determine the optimal set of features.


**Feature Selection: Filtering**\
We first uses information theory to better understand how each feature contributes to information gain. Figure 4.4 presents the four most significant features (left) and the four least significant ones (right).

| features (best)         | i_score  |               | features (worst)         | i_score  |
|--------------------------|-----------|-------------|--------------------------|-----------|
| distance_from_net        | 0.024107  |             | last_event_x             | 0.003538  |
| rebound                  | 0.023004  |             | time_since_last_event    | 0.002954  |
| period                   | 0.021879  |             | period_time_seconds      | 0.002904  |
| y_coord                  | 0.019276  |             | angle_change             | 0.000104  |

- *Figure 4.4. - The best and worst features in terms of information gain.*

\
However, we observe that some significant features appear redundant, such as *distance* and *y_coord*. Therefore, we proceed by testing all possible pairs of features to measure their mutual information. Figure 4.5 shows the pairs with the highest mutual information values.

| combination                                | mi       |   | combination                                | mi       |
|--------------------------------------------|-----------|---|--------------------------------------------|-----------|
| distance_from_net & shot_angle             | 7.019193  |   | y_coord & shot_angle                       | 2.818636  |
| last_event_distance & event_speed          | 4.472923  |   | time_since_last_event & event_speed        | 1.718200  |
| x_coord & distance_from_net                | 4.427126  |   | rebound & angle_change                     | 0.654856  |


- *Figure 4.5. - The pairs sharing the most mutual information.*

\
Based on these observations, we can separate the most relevant features from the redundant and irrelevant ones. However, the distinction is not always clear, so we proceed with a forward wrapper method to test whether the candidate features, as well as the categorical features previously excluded from filtering, improve the performance of the model trained on the relevant features (see Figure 4.6).


| relevance              | features                                                                 |
|------------------------|---------------------------------------------------------------------------|
| relevant features     | distance_from_net / rebound / period / last_event_distance              |
| irrelevant features   | angle_change / time_since_last_event / period_time_seconds / last_event_x |
| redundant features*    | y_coord / x_coord                                                       |
| candidate features     | shot_angle / event_speed / shot_type / last_event_type                  |

- *Figure 4.6. - The best and worst features in terms of information gain.*


\
**Feature Selection: Forward and Backward Wrapper**\
Our base model uses four features. For each feature added from the candidate set, we check whether the model’s performance improves. Based on these results, we select a total seven "final" features whose combinatinon gives the best results: distance_from_net, rebound, period, last_event_distance, shot_angle, shot_type, and period_time_seconds. 

Finally, we run a single backward wrapper starting with all 14 features used in model 2B, removing one feature each time. We observe that removing any of the “final” features significantly worsens the model’s performance, while removing any other feature has only a minimal impact. We therefore proceed to train model 3 using the seven "final" features.



**Model-3**

For model 3, we started from the previous best parameters (from model 2B) and refined the grid. We ran two rounds of cross-validation, adjusting the values, which resulted in the configuration (max_depth=6, eta=0.06, min_child_weight=5, subsample=1, colsample_bytree=0.8) with an AUC of 0.741 after 155 rounds, which we kept for training (figure 4.7). See the relevant WB log entry [here](https://wandb.ai/IFT6758_team4/milestone_2/runs/6rf5t2xm?nw=nwuserantonioslagarias).


<table>
  <tr>
    <td>{% include milestone2/roc_model3.html width="400" %}</td>
    <td>{% include milestone2/percentile_model3.html width="400" %}</td>
  </tr>
  <tr>
    <td>{% include milestone2/cumulative_model3.html width="400" %}</td>
    <td>{% include milestone2/calibration_model3.html width="400" %}</td>
  </tr>
</table>


- *Figure 4.7. - The four figures for model-3. The model's AUC on the validation set is ~0.743*

Model-3’s overall performance is comparable to both Model-2A and Model-2B. Despite using fewer features, it maintains the best AUC achieved by Model 2B, and its cumulative and percentile curves closely resemble those of the best Model-2A. Although the calibration curve is not ideal, its fluctuations are similar to those observed in Model 2B.

---

# **Step 5: Give it your best shot!**

## Question 5.1:
## Question 5.2:



- *Figure 5.1 -*

# **Step 6: Evaluate on test set!**

## Question 6.1:
## Question 6.2:


- *Figure 6.1 -*

---

# Sources / references


